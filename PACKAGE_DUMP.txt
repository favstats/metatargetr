

### FILE:  DESCRIPTION  ###

Package: metatargetr
Title: Retrieve and Parse Meta Ad Targeting Data
Version: 0.0.5
Authors@R: 
    c(person("Fabio", "Votta", , "first.last@example.com", role = c("aut", "cre"),
           comment = c(ORCID = "YOUR-ORCID-ID")),
       person("Philipp", "Mendoza", , "first.last@example.com", role = c("aut"),
           comment = c(ORCID = "YOUR-ORCID-ID")))
Description: The metatargetr package provides tools for retrieving, parsing, and analyzing advertising targeting data from Meta’s Ad Library and Audience Tab.
License: MIT + file LICENSE
VignetteBuilder: quarto
Config/testthat/edition: 3
Encoding: UTF-8
Roxygen: list(markdown = TRUE)
RoxygenNote: 7.3.2
Imports: 
    magrittr,
    glue,
    dplyr,
    purrr,
    jsonlite,
    stringr,
    tibble,
    httr,
    rvest,
    readr,
    tidyr,
    httr2,
    arrow,
    digest,
    rlang,
    xml2,
    lubridate,
    progress,
    cli,
    digest, 
    janitor, 
    vroom,
    pacman
Suggests:
    testthat (>= 3.0.0),
    quarto
URL: https://github.com/favstats/metatargetr
BugReports: https://github.com/favstats/metatargetr/issues


### FILE:  NAMESPACE  ###

# Generated by roxygen2: do not edit by hand

export("%>%")
export(browser_close)
export(download_media)
export(find_name)
export(fix_json)
export(get_ad_html)
export(get_ad_report)
export(get_ad_snapshots)
export(get_additional_page_info_db)
export(get_deeplink)
export(get_ggl_ads)
export(get_linkedin_ads)
export(get_page_info_db)
export(get_page_insights)
export(get_report_db)
export(get_targeting)
export(get_targeting_db)
export(get_targeting_metadata)
export(ggl_get_spending)
export(map_dfr_progress)
export(parse_linkedin_ads_details)
export(parse_location)
export(set_metatargetr_options)
export(set_renv)
export(setup_playwright)
export(stupid_conversion)
export(unnest_and_fix_dups)
export(walk_progress)
import(dplyr)
import(readr)
importFrom(arrow,read_parquet)
importFrom(cli,cli_abort)
importFrom(cli,cli_alert_danger)
importFrom(cli,cli_alert_info)
importFrom(cli,cli_alert_success)
importFrom(dplyr,arrange)
importFrom(dplyr,bind_cols)
importFrom(dplyr,filter)
importFrom(dplyr,left_join)
importFrom(dplyr,mutate)
importFrom(dplyr,mutate_all)
importFrom(dplyr,rename)
importFrom(dplyr,select)
importFrom(dplyr,slice)
importFrom(dplyr,transmute)
importFrom(fs,dir_create)
importFrom(fs,file_move)
importFrom(fs,path_file)
importFrom(glue,glue)
importFrom(httr,GET)
importFrom(httr,content)
importFrom(httr2,req_body_raw)
importFrom(httr2,req_headers)
importFrom(httr2,req_perform)
importFrom(httr2,req_user_agent)
importFrom(httr2,request)
importFrom(httr2,resp_is_error)
importFrom(jsonlite,fromJSON)
importFrom(magrittr,"%>%")
importFrom(purrr,discard)
importFrom(purrr,flatten)
importFrom(purrr,imap_dfr)
importFrom(purrr,is_empty)
importFrom(purrr,map_dfr)
importFrom(purrr,set_names)
importFrom(readr,read_csv)
importFrom(rvest,html_element)
importFrom(rvest,html_elements)
importFrom(rvest,html_text)
importFrom(rvest,html_text2)
importFrom(stringr,str_remove)
importFrom(stringr,str_replace)
importFrom(stringr,str_split)
importFrom(stringr,str_trim)
importFrom(tibble,as_tibble)
importFrom(tibble,tibble)
importFrom(tidyr,separate)
importFrom(utils,unzip)


### FILE:  R/data.R  ###

#' Retrieve Targeting Data from GitHub Repository
#'
#' This function retrieves targeting data for a specific country and timeframe
#' from a GitHub repository hosting parquet files. The function uses the `arrow`
#' package to read the parquet file directly from the specified URL.
#' Note that the retreival of archived data is only possible three days after
#' a specified date.
#'
#' @param the_cntry Character. The ISO country code (e.g., "DE", "US").
#' @param tf Numeric or character. The timeframe in days ("yesterday", "7", "30", "90", "lifelong"). Note, some data points for lifelong in the past may be missing for some countries.
#' @param ds Character. A timestamp or identifier used to construct the file path (e.g., "2024-12-25").
#' @return A data frame containing the targeting data from the parquet file.
#' @importFrom arrow read_parquet
#' @export
#'
#' @examples
#' # Example usage
#' latest_data <- get_targeting_db(
#'   the_cntry = "DE",
#'   tf = 30,
#'   ds = "2024-10-25"
#' )
#' print(head(latest_data))
get_targeting_db <- function(the_cntry, tf, ds, remove_nas = T, verbose = F) {
    # Validate inputs
    if (missing(the_cntry) || missing(tf) || missing(ds)) {
        stop("All parameters (`the_cntry`, `tf`, `ds`) are required.")
    }

    # Construct the URL
    url <- paste0(
        "https://github.com/favstats/meta_ad_targeting/releases/download/",
        the_cntry,        # Country code
        "-last_", tf,     # Timeframe in days
        "_days/",         # Fixed URL segment
        ds,               # Date or identifier
        ".parquet"        # File extension
    )

    if(verbose){
        message("Constructed URL: ", url)
    }
    # Attempt to read the parquet file
    tryCatch({
        data <- arrow::read_parquet(url)
        if(verbose){
            message("Data successfully retrieved.")
        }
        if(remove_nas){
            if("no_data" %in% names(data)){
                data <- data %>% dplyr::filter(is.na(no_data))
                if(verbose){
                    message("Missing data successfully removed.")
                }
            }
        }
        return(data)
    }, error = function(e) {
        stop("Failed to retrieve or parse the parquet file. Error: ", e$message)
    })
}

# # Define example parameters
# the_cntry <- "DE"
# tf <- 30
# ds <- "2024-10-25"
#
# # Call the function
# latest_data <- get_targeting_db(the_cntry, tf, ds)
#
# # Inspect the data
# print(head(latest_data))
# library(tidyverse)
# latest_data %>% filter(is.na(no_data))


#' Retrieve Report Data from GitHub Repository
#'
#' This function retrieves a report for a specific country and timeframe
#' from a GitHub repository hosting RDS files. The file is downloaded
#' to a temporary location, read into R, and then deleted.
#'
#' @param the_cntry Character. The ISO country code (e.g., "DE", "US").
#' @param timeframe Character or Numeric. Timeframe in days (e.g., "30", "90") or "yesterday" / "lifelong".
#' @param ds Character. A timestamp or identifier used to construct the file name (e.g., "2024-12-25").
#' @param verbose Logical. Whether to print messages about the process. Default is `FALSE`.
#' @return A data frame or object read from the RDS file.
#' @export
#'
#' @examples
#' # Example usage
#' report_data <- get_report_db(
#'   the_cntry = "DE",
#'   timeframe = 30,
#'   ds = "2024-12-25",
#'   verbose = TRUE
#' )
#' print(head(report_data))
get_report_db <- function(the_cntry, timeframe, ds, verbose = FALSE) {
    # Validate inputs
    if (missing(the_cntry) || missing(timeframe) || missing(ds)) {
        stop("All parameters (`the_cntry`, `timeframe`, `ds`) are required.")
    }

    # Construct the timeframe string
    if (is.numeric(timeframe)) {
        tf_string <- paste0("-last_", timeframe, "_days")
    } else if (timeframe %in% c("yesterday", "lifelong")) {
        tf_string <- paste0("-", timeframe)
    } else {
        stop("Invalid `timeframe` value. Must be numeric (e.g., 30, 90) or 'yesterday' / 'lifelong'.")
    }

    # Construct the file name
    file_name <- paste0(ds, ".rds")

    if(as.Date(ds) > as.Date("2025-07-05")){
        # Construct the URL
        url <- paste0(
            "https://github.com/favstats/meta_ad_reports2/releases/download/",
            the_cntry, tf_string, "/",
            file_name
        )
    } else {
        # Construct the URL
        url <- paste0(
            "https://github.com/favstats/meta_ad_reports/releases/download/",
            the_cntry, tf_string, "/",
            file_name
        )
    }


    # Temporary file path
    temp_file <- tempfile(fileext = ".rds")

    if (verbose) {
        message("Constructed URL: ", url)
        message("Downloading to temporary file: ", temp_file)
    }

    # Attempt to download and read the RDS file
    tryCatch({
        download.file(url, destfile = temp_file, mode = "wb")
        if (verbose) {
            message("File successfully downloaded.")
        }

        # Read the RDS file
        data <- readRDS(temp_file)
        if (verbose) {
            message("Data successfully read from the RDS file.")
        }

        # Return the data
        return(data)
    }, error = function(e) {
        stop("Failed to retrieve or parse the RDS file. Error: ", e$message)
    }, finally = {
        # Ensure the temporary file is deleted
        if (file.exists(temp_file)) {
            file.remove(temp_file)
            if (verbose) {
                message("Temporary file deleted.")
            }
        }
    })
}


# report_data <- get_report_db(
#   the_cntry = "DE",
#   timeframe = 7,
#   ds = "2024-10-25",
#   verbose = TRUE
# )


#' Retrieve Metadata for Targeting Data
#'
#' This function retrieves metadata for targeting data releases for a specific
#' country and timeframe from a GitHub repository.
#'
#' @param country_code Character. The ISO country code (e.g., "DE", "US").
#' @param timeframe Character. The timeframe to filter (e.g., "7", "30", or "90").
#' @param base_url Character. The base URL for the GitHub repository. Defaults to
#' `"https://github.com/favstats/meta_ad_targeting/releases/"`.
#' @return A data frame containing metadata about available targeting data,
#' including file names, sizes, timestamps, and tags.
#' @importFrom httr GET content
#' @importFrom rvest html_elements html_text
#' @importFrom dplyr transmute mutate filter rename arrange
#' @importFrom tidyr separate
#' @importFrom tibble tibble
#' @export
#'
#' @examples
#' # Retrieve metadata for Germany for the last 30 days
#' metadata <- get_targeting_metadata("DE", "30")
#' print(metadata)
get_targeting_metadata <- function(country_code,
                                        timeframe,
                                        base_url = "https://github.com/favstats/meta_ad_targeting/releases/expanded_assets/") {
    # Validate inputs
    if (missing(country_code)) {
        stop("Parameter `country_code` is required.")
    }


    if(timeframe %in% c(7, "LAST_7_DAYS")){
        timeframe <- "7"
    } else if(timeframe %in% c(30, "LAST_30_DAYS")){
        timeframe <- "30"
    } else if(timeframe %in% c(90, "LAST_90_DAYS")){
        timeframe <- "90"
    }

    # Timeframe suffix for filtering
    timeframe_suffix <- paste0("-last_", timeframe, "_days")

    # Construct the full URL
    url <- paste0(base_url, country_code, timeframe_suffix)

    # Fetch the data
    response <- httr::GET(url)

    if (httr::status_code(response) != 200) {
        stop("Failed to retrieve metadata from: ", url, ". Status code: ", httr::status_code(response))
    }

    html_content <- xml2::read_html(httr::content(response, as = "text", encoding = "UTF-8"))

    raw_elements <- rvest::html_elements(html_content, ".Box-row") %>%
        rvest::html_text()

    metadata <- tibble::tibble(raw = raw_elements) %>%
        dplyr::mutate(raw = strsplit(as.character(raw), "\n")) %>%
        dplyr::transmute(
            filename = sapply(raw, function(x) trimws(x[3])),
            file_size = sapply(raw, function(x) trimws(x[6])),
            timestamp = sapply(raw, function(x) trimws(x[7]))
        ) %>%
        dplyr::filter(filename != "Source code") %>%
        dplyr::mutate(release = paste0(country_code, timeframe_suffix)) %>%
        dplyr::mutate_all(as.character) %>%
        dplyr::rename(tag = release, file_name = filename) %>%
        dplyr::arrange(desc(tag)) %>%
        tidyr::separate(tag, into = c("cntry", "tframe"), sep = "-", remove = FALSE) %>%
        dplyr::mutate(ds = stringr::str_remove(file_name, "\\.rds|\\.zip|\\.parquet")) %>%
        dplyr::distinct(cntry, ds, tframe) %>%
        tidyr::drop_na(ds) %>%
        dplyr::arrange(desc(ds))

    return(metadata)
}


### FILE:  R/env.R  ###

## functions copied from rtweet (https://raw.githubusercontent.com/ropensci/rtweet/f9cada85b60101abce14c19afb8c4707202ad699/R/renv.R)

.Renviron <- function() {
    if (file.exists(".Renviron")) {
        ".Renviron"
    } else {
        file.path(home(), ".Renviron")
    }
}

home <- function() {
    if (!identical(Sys.getenv("HOME"), "")) {
        file.path(Sys.getenv("HOME"))
    } else {
        file.path(normalizePath("~"))
    }
}


is_named <- function(x) !is.null(names(x))

are_named <- function(x) is_named(x) && !"" %in% names(x)

readlines <- function(x, ...) {
    con <- file(x)
    x <- readLines(con, warn = FALSE, ...)
    close(con)
    x
}

define_args <- function(args, ...) {
    dots <- list(...)
    nms <- names(dots)
    for (i in nms) {
        if (!has_name_(args, i)) {
            args[[i]] <- dots[[i]]
        }
    }
    args
}

append_lines <- function(x, ...) {
    args <- define_args(
        c(x, list(...)),
        append = TRUE,
        fill = TRUE
    )
    do.call("cat", args)
}

is_incomplete <- function(x) {
    con <- file(x)
    x <- tryCatch(readLines(con), warning = function(w) return(TRUE))
    close(con)
    ifelse(isTRUE(x), TRUE, FALSE)
}

clean_renv <- function(var) {
    x <- readlines(.Renviron())
    x <- grep(sprintf("^%s=", var), x, invert = TRUE, value = TRUE)
    writeLines(x, .Renviron())
}

check_renv <- function(var = NULL) {
    if (!file.exists(.Renviron())) return(invisible())
    if (is_incomplete(.Renviron())) {
        append_lines("", file = .Renviron())
    }
    if (!is.null(var)) {
        clean_renv(var)
    }
    invisible()
}

#' @export
set_renv <- function(...) {
    dots <- list(...)
    stopifnot(are_named(dots))
    vars <- names(dots)
    x <- paste0(names(dots), "=", dots)
    x <- paste(x, collapse = "\n")
    for (var in vars) {
        check_renv(var)
    }
    append_lines(x, file = .Renviron())
    readRenviron(.Renviron())
}

has_name_ <- function(x, name) isTRUE(name %in% names(x))


### FILE:  R/get_ad_html.R  ###

    # get_ad_html_many() ---------------------------------------------------------------------------
    #' Fetch many Facebook-Ad-Library pages (vectorised, cached, parallel)
    #'
    #' @param ad_ids         Character vector of Ad-Library IDs.
    #' @param country        Two-letter country code.
    #' @param cache_dir      Directory where *.html.gz* files will be stored.
    #'                       Defaults to the value set during interactive setup,
    #'                       or "html_cache".
    #' @param overwrite      If FALSE (default) keep already-cached files.
    #' @param strip_css      Run the same fast, regex-based CSS removal as the
    #'                       single-ID helper **only on newly-downloaded pages**.
    #' @param max_active     Maximum number of concurrent sockets passed to
    #'                       `httr2::req_perform_parallel()` (default = 8).
    #' @param ua             User-Agent string. If NULL (default), uses a standard
    #'                       or randomized UA based on `randomize_ua`.
    #' @param randomize_ua   Boolean. If TRUE, a random User-Agent is chosen from
    #'                       a predefined list for each request to make it harder
    #'                       to track. Defaults to the value set during
    #'                       interactive setup, or FALSE.
    #' @param log_failed_ids If a character path is provided (e.g., "log.txt"),
    #'                       a log of failed IDs will be written
    #'                       to that file. Default is NULL (no log file).
    #' @param timeout_sec, retries
    #'                       Passed through to the underlying requests.
    #' @param quiet          Suppress progress messages.
    #' @param return_type    `"paths"` (default) or `"list"` for in-memory strings.
    #' @param interactive    If TRUE, run a one-time interactive setup to configure
    #'                       and save default settings. Default is FALSE.
    #'
    #' @return Either a named character vector of file paths or a named list of
    #'         HTML strings, in the *same order* as `ad_ids`.
    #' @export
    get_ad_html <- function(ad_ids,
                                 country,
                                 cache_dir = NULL,
                                 overwrite = FALSE,
                                 strip_css = TRUE,
                                 max_active = 8,
                                 log_failed_ids = NULL,
                                 # Default UA is now handled by options
                                 ua = NULL,
                                 # New arguments start here
                                 randomize_ua = NULL,
                                 interactive = FALSE,
                                 # End new arguments
                                 timeout_sec = 15,
                                 retries = 3L,
                                 quiet = FALSE,
                                 return_type = c("paths", "list")) {

        ## 0. One-time Interactive Setup (using cli) ------------------------------
        # Only runs when interactive = TRUE and settings haven't been configured
        if (interactive && !isTRUE(getOption("metatargetr.configured"))) {



            cli::cli_h1("Welcome! Let's configure your settings.")
            cli::cli_text("This will set session-wide defaults for {.code get_ad_html}.")

            # Ask for cache directory, allowing user to skip
            cli::cli_alert_info("You can type {.val skip} at the first prompt to use the default settings.")

            # Construct the prompt manually with cli and readline
            cli::cli_text("{.field First, where should we save the downloaded HTML files?}")
            chosen_dir_raw <- readline(prompt = cli::format_inline("Cache directory (by default: {.path html_cache}): "))

            # Use default if user just presses enter, otherwise use their input
            chosen_dir <- if (identical(chosen_dir_raw, "")) "html_cache" else chosen_dir_raw

            # Check if the user wants to skip
            if (identical(chosen_dir, "skip")) {

                # Set default options
                options(metatargetr.cache_dir = "html_cache")
                options(metatargetr.randomize_ua = FALSE)
                cli::cli_alert_info("Skipping setup and using default settings.")

            } else {

                # Proceed with full configuration
                options(metatargetr.cache_dir = chosen_dir)
                cli::cli_alert_success("Cache directory set to: {.path {chosen_dir}}")

                # Ask about randomizing user agents
                cli::cli_div(theme = list(body = list(`margin-top` = 1))) # Add a blank line
                cli::cli_text("{.field To make scraping more robust, you can randomize the User-Agent for each request.}")
                random_pref <- cli_ask_yes_no("Would you like to enable randomized User-Agents by default?")
                options(metatargetr.randomize_ua = random_pref)
                cli::cli_alert_success("Randomize User-Agents set to: {.val {random_pref}}")

                # Ask to save settings permanently
                cli::cli_div(theme = list(body = list(`margin-top` = 1)))
                cli::cli_text("{.field These settings apply to your current R session.}")
                save_pref <- cli_ask_yes_no("Do you want to save settings to your {.path .Renviron} file for future sessions? (recommended)")

                if (save_pref) {
                    set_renv("METATARGETR_CACHE_DIR" = chosen_dir)
                    set_renv("METATARGETR_RANDOMIZE_UA" = random_pref)

                }

                cli::cli_rule(left = "Configuration complete!")
            }

            # Mark as configured for this session to avoid asking again
            options(metatargetr.configured = TRUE)
        }
        ## 1. Set Defaults from Options -------------------------------------------
        # Use session options if arguments are not provided by the user
        if (is.null(cache_dir)) {
            cache_dir <- getOption("metatargetr.cache_dir", "html_cache")
        }
        if (is.null(randomize_ua)) {
            randomize_ua <- getOption("metatargetr.randomize_ua", FALSE)
        }
        # Default User-Agent if not randomizing and none is provided
        default_ua <- "metatargetr"
        if (is.null(ua) && !randomize_ua) {
            ua <- default_ua
        }

        ## 2. Input checks --------------------------------------------------------
        stopifnot(length(country) == 1, grepl("^[A-Za-z]{2}$", country))
        stopifnot(is.character(ad_ids) && length(ad_ids) > 0)
        return_type <- match.arg(return_type)


        ## 3. Set up cache --------------------------------------------------------
        fs::dir_create(cache_dir, recurse = TRUE)
        safe_ids <- gsub("[^A-Za-z0-9_\\-]", "_", ad_ids)
        filepaths <- fs::path(cache_dir, sprintf("%s_%s.html.gz", country, safe_ids))
        names(filepaths) <- ad_ids

        need <- !(fs::file_exists(filepaths) & !overwrite)
        ids_dl <- ad_ids[need]
        paths_dl <- filepaths[need]

        if (!quiet) {
            message(
                "✓ ", sum(!need), " already on disk, ",
                length(ids_dl), " to download …"
            )
        }


        ## early exit: nothing left to do -----------------------------------------
        if (length(ids_dl) == 0) {
            return(if (return_type == "paths") {
                filepaths
            } else {
                lapply(filepaths, read_gz_char)
            })
        }

        ## 4. Build a vector of httr2 requests -----------------------------------

        reqs <- ids_dl %>% purrr::map(~build_req(.x, country, randomize_ua, ua, timeout_sec, retries))

        ## 5. Fire the requests in parallel ---------------------------------------

        if (!quiet) {
            cli::cli_alert_info("Making requests...")
        }


        resps <- httr2::req_perform_parallel(
            reqs,
            on_error = "continue",
            progress = !quiet,
            max_active = max_active
        )



        ## 6. Post-process each response ------------------------------------------
        html_vec <- character(length(ids_dl))

        # Initialize the progress bar if not in quiet mode
        if (!quiet) {
            cli::cli_alert_info("Processing HTMLs... may take a while.")
            cli::ansi_with_hidden_cursor(fun_with_spinner2())
            cli::cli_progress_bar(
                "Processing...",
                total = length(resps),
                clear = FALSE # Keep the bar after finishing
            )
        }


        for (i in seq_along(resps)) {
            # (Inside the for-loop)
            resp <- resps[[i]]
            current_id <- ids_dl[i]

            # Check for a low-level request error first
            if (inherits(resp, "error")) {
                reason <- conditionMessage(resp)
                # Display warning without interrupting the progress bar
                if (!quiet) cli::cli_alert_warning(c("x" = "Request for ID {.val {current_id}} failed.", "!" = "Reason: {reason}"))
                html_vec[i] <- NA_character_
                if (!quiet) cli::cli_progress_update() # Advance progress bar
                next
            }

            # If the request succeeded, check for an HTTP error status code
            if (httr2::resp_status(resp) >= 400) {
                reason <- httr2::resp_status_desc(resp)
                if (!quiet) cli::cli_alert_warning(c("x" = "HTTP error for ID {.val {current_id}}.", "!" = "Status: {.strong {httr2::resp_status(resp)} {reason}}"))
                html_vec[i] <- NA_character_
                if (!quiet) cli::cli_progress_update() # Advance progress bar
                next
            }

            html_raw <- httr2::resp_body_string(resp)

            if (strip_css) {
                html_raw <- gsub("(?is)<style[^>]*>.*?</style>", "", html_raw, perl = TRUE)
                html_raw <- gsub("(?is)<link[^>]*?rel=[\"']?stylesheet[\"']?[^>]*>", "",
                                 html_raw,
                                 perl = TRUE
                )
            }

            con <- gzfile(paths_dl[i], "wb")
            writeBin(charToRaw(html_raw), con)
            close(con)

            html_vec[i] <- html_raw

            # Add a success message for verbose output
            if (!quiet) {
                # Using cli_alert_success for nicely formatted output
                # Advance the progress bar
                cli::cli_progress_update()
            }
        }

        # Ensure the progress bar is marked as done
        # if (!quiet) {
        #     cli::cli_progress_done()
        # }


        # 7. Assemble results in original order ----------------------------------
        out <- vector("list", length(ad_ids))
        names(out) <- ad_ids

        out[ids_dl] <- if (return_type == "paths") paths_dl else html_vec

        cached_ids <- ad_ids[!need]
        if (length(cached_ids) > 0) {
            if (return_type == "paths") {
                out[cached_ids] <- filepaths[cached_ids]
            } else {
                out[cached_ids] <- lapply(filepaths[cached_ids], read_gz_char)
            }
        }

        # 8. Final Report and Logging ----------------------------------------------

        # Identify final successes and failures from the complete output
        successful_ids <- dir(cache_dir) %>%
            str_remove_all(country) %>%
            str_remove_all(".html.gz|_") %>%
            keep(~.x %in% ad_ids)
        failed_ids <- setdiff(successful_ids, ad_ids)
        n_success <- length(successful_ids)
        n_fail <- length(failed_ids)

        # Display a summary report in the console
        if (!quiet) {
            cli::cli_rule(left = "Download Summary")
            cli::cli_alert_success("{n_success} ID(s) processed successfully.")
            if (n_fail > 0) {
                cli::cli_alert_danger("{n_fail} ID(s) failed.")
            }
        }

        # Write to a log file if a path is provided
        if (!is.null(log_failed_ids) & length(failed_ids) != 0) {
            tryCatch(
                {
                    cat(failed_ids, file = log_failed_ids, append = TRUE, sep = "\n")

                    if (!quiet) cli::cli_alert_info("Failed IDs written to {.path {log_failed_ids}}")
                },
                error = function(e) {
                    if (!quiet) cli::cli_warn("Failed to write log file: {conditionMessage(e)}")
                }
            )
        }

        # 9. return statements

        if (return_type == "paths") {
            # Ensure consistent return type (named char vector) and order
            return(unlist(out)[ad_ids])
        } else {
            # Ensure list is in the correct order
            return(out[ad_ids])
        }
    }

    # ids <- readLines("dev/AU_ids.txt")
    # #
    # # library(tidyverse)
    # # # debugonce(get_ad_html)
    # #
    # get_ad_html(sample(ids, 100), country = "AU",
    #                  interactive = F, log_failed_ids = "log.txt", overwrite = F, quiet = F) -> hi



### FILE:  R/get_ad_report.R  ###

#' Closes a Playwright browser instance
#'
#' @description
#' This function safely closes the browser instance associated with the provided
#' browser object. It's designed to handle cases where the browser may have
#' already been closed, preventing errors.
#'
#' @param browser_df A tibble returned by `browser_launch()`, which contains
#'   the `browser_id` of the instance to be closed.
#'
#' @return Invisibly returns `NULL`. This function is called for its side effect
#'   of closing the browser.
#'
#' @examples
#' \dontrun{
#' # Launch a browser
#' browser <- browser_launch()
#'
#' # ... perform actions with the browser ...
#'
#' # Close the browser instance when done
#' browser_close(browser)
#' }
#'
#' @importFrom glue glue
#' @export
browser_close <- function(browser_df) {
  # Validate that the input is the correct object
  if (!is.data.frame(browser_df) || !"browser_id" %in% names(browser_df)) {
    stop("Input must be the tibble object returned by `browser_launch()`.")
  }

  browser_id <- browser_df$browser_id

  # Construct the Python command to close the browser context
  close_command <- glue::glue("{browser_id}.close()")

  # Execute the command, wrapping it in try() to suppress
  # errors if the browser target has already been closed.
  try(
    playwrightr:::py_run(close_command),
    silent = TRUE
  )

  # Return NULL invisibly, as the function is used for its side effect
  invisible(NULL)
}

# Helper functions for playwrightr
on <- function(page_df, event, lambda_string) {
  the_page_id <- page_df$page_id
  playwrightr:::py_run(glue::glue('{the_page_id}.on("{event}", {lambda_string})'))
  return(page_df)
}
off <- function(page_df, event, lambda_string) {
  the_page_id <- page_df$page_id

  playwrightr:::py_run(glue::glue(
    '{the_page_id}.remove_listener("{event}", {lambda_string})'
  ))
  return(page_df)
}

execute_script <- function (page_df, script) {
  the_page_id <- page_df$page_id

  playwrightr:::py_run(glue("d = {{the_page_id}}.evaluate('{{script}}')"))
}

#' Set up and initialize the playwrightr environment
#'
#' @keywords internal
#' @export
setup_playwright <- function() {
  if (!require("pacman")) install.packages("pacman")
  pacman::p_load(reticulate, playwrightr, cli)
  cli_h2("Checking Playwright Setup")
  if (!py_module_available("playwright")) {
    cli_alert_info("Python 'playwright' module not found. Installing now...")
    tryCatch({
      py_install("playwright", pip = TRUE)
    }, error = function(e) {
      stop("Failed to install 'playwright' Python module.")
    })
  }
  cli_alert_info("Checking for browser binaries...")
  tryCatch({
    system("playwright install --force")
  }, error = function(e) {
    stop("Failed to install playwright browser binaries.")
  })
  cli_alert_info("Initializing Playwright...")
  tryCatch({
    if (Sys.info()[["sysname"]] == "Darwin") {
      pw_init(use_xvfb = FALSE)
    } else {
      if (!py_module_available("xvfbwrapper")) {
        py_install("xvfbwrapper", pip = TRUE)
      }
      pw_init(use_xvfb = TRUE)
    }
    cli_alert_success("Playwright initialized successfully.")
  }, error = function(e) {
    stop("Playwright initialization failed. Error: ", e$message)
  })
  invisible(TRUE)
}

#' Get Facebook Ad Library Report Data
#'
#' Automates downloading Facebook Ad Library reports for vectors of countries,
#' timeframes, and dates. It uses a robust tryCatch block for each request
#' to ensure cleanup and prevent hanging processes.
#'
#' @param country A character vector of two-letter ISO country codes.
#' @param timeframe A character vector of time windows (e.g., "last_7_days").
#' @param date A character vector of report dates in "YYYY-MM-DD" format.
#'
#' @return A single tibble containing the combined data for all successful requests.
#' @export
get_ad_report <- function(country, timeframe, date) {

  # --- 1. SETUP ---
  cli_h1("Step 1: Setting up Environment")
  if (!require("pacman")) install.packages("pacman")
  if (!"playwrightr" %in% installed.packages()) remotes::install_github("benjaminguinaudeau/playwrightr")
  if (!"purrr" %in% installed.packages()) install.packages("purrr")


  setup_playwright()

  # --- 2. INITIALIZATION & TEMPLATE CAPTURE ---
  cli_h1("Step 2: Initializing Browser and Capturing API Template")
  browser_df <- browser_launch(headless = TRUE, browser = "firefox")
  on.exit(browser_df %>% browser_close(), add = TRUE)

  page_df <- browser_df # Use the main page from the persistent context
  page_df %>% goto("https://www.facebook.com/ads/library/report")
  Sys.sleep(3)
  try(page_df %>% get_by_test_id("cookie-policy-manage-dialog-accept-button") %>% slice(1) %>% click(), silent = TRUE)

  tmp_post_data_string <- tempfile(fileext = ".txt")
  page_df %>% on("request", glue('lambda request: open("{tmp_post_data_string}", "w").write(request.post_data) if (request.method == "POST" and "graphql" in request.url) else None'))
  page_df %>% get_by_text("Download report") %>% slice(2) %>% click()
  Sys.sleep(4)
  data_string <- readLines(tmp_post_data_string, warn = FALSE) %>% str_squish()
  unlink(tmp_post_data_string)

  if (length(data_string) == 0) {
    cli_alert_danger("Fatal: Could not capture the initial API request data. Cannot proceed.")
    return(NULL)
  }

  # *** FIX: Create a clean template by removing the original variables ***
  data_string_template <- stringr::str_replace(data_string, "&variables=%7B.*?%7D", "")

  tmp_download_link <- tempfile(fileext = ".txt")
  page_df %>% on("console", glue::glue("lambda msg: open('{tmp_download_link}', 'w').write(msg.text)"))

  # --- 3. EXECUTION LOOP ---
  request_grid <- tidyr::expand_grid(country, timeframe, date)
  cli_h1("Step 3: Processing {nrow(request_grid)} Report(s)")

  results <- purrr::map_dfr(split(request_grid, 1:nrow(request_grid)), function(params) {

    # Use standard assignment, not global assignment (<<-)
    current_country <- params$country
    current_timeframe <- params$timeframe
    current_date <- params$date

    cli_h2("Requesting: {current_country} | {current_timeframe} | {current_date}")

    # *** FIX: Use the clean template and append new variables ***
    js_code <- paste0(
      'fetch("https://www.facebook.com/api/graphql/", {"headers": {"accept": "*/*", "content-type": "application/x-www-form-urlencoded"}, "body": "',
      # Use the template here
      data_string_template,
      # Append the new, correct variables for this iteration
      "&variables=%7B%22country%22%3A%22", current_country ,"%22%2C%22reportDS%22%3A%22", current_date ,"%22%2C%22timePreset%22%3A%22", current_timeframe,"%22%7D",
      '", "method": "POST", "mode": "cors", "credentials": "include" }).then(resp => resp.text()).then(data => console.log(data));'
    )

    page_df %>% execute_script(js_code)
    Sys.sleep(5)

    # Read and process download link
    download_url <- readLines(tmp_download_link, warn = FALSE) %>% str_extract("\"https.*?\"") %>% str_remove_all("(^\"|(\"|\\\\)$)")

    if (is.na(download_url) || str_detect(download_url, "facebook.com/help/contact/")) {
      cli_alert_danger("Failed to get a download link for {current_country} on {current_date}. Skipping.")
      return(NULL)
    }

    # --- Data Processing ---
    temp_zip_file <- tempfile(fileext = ".zip")
    download.file(download_url, temp_zip_file, mode = "wb", quiet = TRUE)
    temp_extract_dir <- tempfile()
    unzip(temp_zip_file, exdir = temp_extract_dir)
    csv_path <- dir(temp_extract_dir, pattern = "advertisers.csv", full.names = TRUE, recursive = TRUE)

    if (length(csv_path) == 0) {
      cli_alert_warning("Report ZIP for {current_country} on {current_date} was empty. Skipping.")
      unlink(c(temp_zip_file, temp_extract_dir), recursive = TRUE)
      return(NULL)
    }

    report_data <- vroom(csv_path[1], show_col_types = FALSE) %>%
      clean_names() %>%
      mutate(
        report_date = current_date,
        report_timeframe = current_timeframe,
        report_country = current_country
      )

    unlink(c(temp_zip_file, temp_extract_dir), recursive = TRUE)
    cli_alert_success("Successfully processed report for {current_country} on {current_date}.")

    return(report_data)
  })

  # The page is part of the persistent context and is closed when the browser is closed
  # No need for a separate page_df %>% close_page() call

  cli_h1("All Requests Complete")
  return(results)
}

# debugonce(get_ad_report)
# report_data <- get_ad_report(
#   country = "US",
#   timeframe = "lifelong",
#   date = c("2025-01-20")
# )
#
# report_data <- get_ad_report(
#   country = c("US"),
#   timeframe = "lifelong",
#   date = c("2025-01-20", "2025-06-30")
# )


### FILE:  R/get_ads_info.R  ###

#' @title Get and Parse Ad Library Data
#'
#' @description
#' A wrapper function that downloads ad HTMLs for a given set of IDs and
#' a country, parses the data, and returns a final, reordered dataframe.
#'
#' @param ad_ids A character vector of Ad Library IDs.
#' @param country A two-letter country code.
#' @param keep_html A logical flag. If `FALSE` (the default), the cache
#'   directory with the downloaded HTML files will be deleted after parsing.
#'   If `TRUE`, the files will be kept.
#' @param cache_dir The directory to store downloaded HTML files. Defaults to
#'   "html_cache".
#' @param ... Additional arguments to be passed down to `get_ad_html()`
#'   (e.g., `overwrite`, `quiet`, `max_active`).
#'
#' @return A single, reordered dataframe containing the parsed ad data.
#'
get_ads_info <- function(ad_ids, country, keep_html = TRUE, cache_dir = "html_cache", ...) {

    # --- 1. Download Step ---
    # Use your existing get_ad_html function to download the files.
    # It will show progress and handle caching.
    # We force return_type to "paths" because the parser reads from disk.
    cli::cli_h1("Step 1: Downloading HTML Files")
    get_ad_html(
        ad_ids = ad_ids,
        country = country,
        cache_dir = cache_dir,
        return_type = "paths", # Force path return for the parser
        ... # Pass along any other user-defined arguments
    )

    # --- 2. Parsing Step ---
    # Use your existing parse_ad_htmls function to process the downloaded files.
    # It will show its own progress and return the final dataframe.
    cli::cli_h1("Step 2: Parsing HTML and Extracting Data")
    parsed_df <- parse_ad_htmls(html_dir = cache_dir)
    cli::cli_alert_success("Successfully parsed {nrow(parsed_df)} ads.")


    # --- 3. Cleanup Step ---
    # If keep_html is FALSE, delete the cache directory.
    if (!keep_html) {
        cli::cli_h1("Step 3: Cleaning Up")
        tryCatch({
            unlink(cache_dir, recursive = TRUE, force = TRUE)
            cli::cli_alert_info("Cache directory '{.path {cache_dir}}' has been deleted.")
        }, error = function(e) {
            cli::cli_alert_warning("Could not delete cache directory: {e$message}")
        })
    } else {
        cli::cli_alert_info("Keeping HTML files in cache directory: '{.path {cache_dir}}'")
    }

    cli::cli_rule(left = "Process Complete!")

    # --- 4. Return Final Dataframe ---
    return(parsed_df)
}



# get_ads_info(sample(ids, 100), country = "AU",
#             interactive = F, log_failed_ids = "log.txt", overwrite = F, quiet = F) -> hi


### FILE:  R/get_description WIP  ###


library(tidyverse)

page_id_dat <- readRDS("data/page_id_dat.rds")

ad_library_about <- function(the_url, reconnect = F) {
    remDr$navigate(the_url)
    Sys.sleep(0.5)
    # Get the webpage HTML
    # html <<- rvest::session(url) %>% rvest::read_html()
    # chrr <- as.character(html)
    # chrr <- httr::GET(url) %>% .[["content"]] %>% rawToChar
    thth <- remDr$getPageSource() %>% .[[1]] %>% read_html()
    chrr <- as.character(thth)
    # print("11")
    if(str_detect(chrr, "Page has been deleted|unpublished")) {
        write_csv(tibble(ad_library_url = the_url), "data/deleted_pages.csv", append = T)
    }

    if(reconnect & str_detect(chrr, "temporarily blocked")){
        print("reconnect..")
        system2("powershell.exe", args = c("-Command", "Restart-NetAdapter -Name \"WiFi\""), wait = TRUE, invisible = FALSE)
        Sys.sleep(60*1)
        # counter <<- 0
    }

    # print("22")
    # print(chrr)
    str_extract(chrr, 'about:\\{.*?\\}')

}


library(RSelenium)

# https://adstransparency.google.com/advertiser/AR09355418985304162305?political&region=NL&preset-date=Last%207%20days


library(netstat)
library(RSelenium)

# Randomize the user agent
ua_list <- c(
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:92.0) Gecko/20100101 Firefox/92.0"
)
ua <- sample(ua_list, 1)

# Randomize other browser options
disable_images <- sample(c(TRUE, FALSE), 1)
disable_javascript <- sample(c(TRUE, FALSE), 1)

# Start the Selenium server with random options
podf <- sample(4000L:5000L, 1)
rD <- rsDriver(
    browser = "firefox",
    chromever = NULL,
    check = FALSE,
    port = podf,
    verbose = TRUE,
    extraCapabilities = list(
        firefoxOptions = list(
            args = c(
                "-headless",
                "-disable-gpu",
                "-disable-extensions",
                "-mute-audio",
                "-no-sandbox",
                "-disable-dev-shm-usage",
                paste0("-user-agent='", ua, "'"),
                paste0("-blink-settings=imagesEnabled=", !disable_images),
                paste0("-javascriptEnabled=", !disable_javascript)
            )
        )
    )
)


remDr <- rD$client

library(rvest)


# url <- "https://www.facebook.com/ads/library/?active_status=all&ad_type=political_and_issue_ads&country=US&view_all_page_id=7860876103&sort_data[direction]=desc&sort_data[mode]=relevancy_monthly_grouped&search_type=page&media_type=all"


# ad_library_about("https://www.facebook.com/ads/library/?active_status=all&ad_type=political_and_issue_ads&country=US&view_all_page_id=114706870277904&search_type=page&media_type=all")

# (html)

# ad_library_about("https://www.facebook.com/ads/library/?active_status=all&ad_type=political_and_issue_ads&country=FR&view_all_page_id=100129489256827&search_type=page&media_type=all")

# ad_library_about("https://www.facebook.com/ads/library/?active_status=all&ad_type=political_and_issue_ads&country=US&view_all_page_id=7860876103&sort_data[direction]=desc&sort_data[mode]=relevancy_monthly_grouped&search_type=page&media_type=all")

# get_proxy <- function(){
#   if(Sys.getenv("BINANCE_PROXY") == "") return("")
#
#   stringr::str_split(Sys.getenv("BINANCE_PROXY"), ":")[[1]]
# }
#
# get_proxy_user <- function(){
#   if(Sys.getenv("BINANCE_USERPW") == "") return("")
#
#   stringr::str_split(Sys.getenv("BINANCE_USERPW"), ":")[[1]]
# }
#
# add_proxy <- function(){
#   httr::use_proxy(url = get_proxy()[1],
#                   port = as.numeric(get_proxy()[2]),
#                   username = get_proxy_user()[1],
#                   password = get_proxy_user()[2])
# }

# tmp_res <- httr::GET(url,
#                      add_proxy(),
#                      httr::add_headers(.headers = account_headers()))


ad_library_about_pos <- possibly(ad_library_about, otherwise = NA, quiet = F)


get_new <- function(page_id_dat, reconnect = F) {

    # yooo <- c(
    #   readRDS("where_abouts.rds") %>% drop_na() %>% .$page_id,
    #   where_abouts2 %>% drop_na() %>% .$page_id
    # ) %>% unique()

    where_abouts2 <<- page_id_dat %>%
        sample_n(n()) %>%
        split(1:nrow(.)) %>%
        map_dfr_progress(~{
            # browser()
            # print("1")
            this_is_it <<- tibble(page_id = .x$page_id,
                                  about_text = ad_library_about_pos(.x$ad_library_link, reconnect))

            # print("2")
            if(is.na(this_is_it$about_text)){
                counter <<- counter + 1
                if(counter %% 5 == 0){
                    print(paste0("its NA: ", counter))
                }
            } else {
                counter <<- 0
                saveRDS(this_is_it, paste0("data/abouts/", .x$page_id, ".rds"))
            }

            if(counter > 100){
                # print("reconnect..")
                # system2("powershell.exe", args = c("-Command", "Restart-NetAdapter -Name \"WiFi\""), wait = TRUE, invisible = FALSE)
                # Sys.sleep(60*1)
                # counter <<- 0
            }
        })
    # filter(!(page_id %in% yooo)) %>%
    # sample_n(10) %>%
    # rowwise() %>%
    # mutate(about_text = ad_library_link %>% map_chr_progress(ad_library_about_pos)) %>%
    #   # ungroup() %>%
    #   select(page_id, about_text)
    #
    # print(nrow(where_abouts2))

    # saveRDS(where_abouts %>% bind_rows(where_abouts2) %>% distinct(), file = "where_abouts.rds")

}


counter <- 0

library(tidyverse)

already_there <- dir("data/abouts") %>%
    str_remove_all("\\.rds")
#
# rstudioapi::jobRunScript("get_about_text3.R")
# rstudioapi::jobRunScript("get_about_text4.R")
# rstudioapi::jobRunScript("get_about_text5.R")

# library(furrr)
# library(future)

# plan(multisession, workers = 10)

# batch_name <- 0

# page_id_dat %>%
#   distinct(page_id, .keep_all = T) %>%
#   # slice(140001:n()) %>%
#   filter((page_id %in% about_dat$page_id)) %>%
#   filter((page_id %in% already_there))

about_dat <- readRDS("data/about_dat.rds")

deleted_pages <- read_csv("data/deleted_pages.csv") %>% distinct() %>%
    mutate(page_id = str_extract(ad_library_url, "view_all_page_id=.*&search") %>%
               str_remove_all("view_all_page_id=|&search"))

# sdo <- "https://www.facebook.com/ads/library/?active_status=all&ad_type=political_and_issue_ads&country=FR&view_all_page_id=100129489256827&search_type=page&media_type=all"


splitted <- page_id_dat %>%
    # slice(1:80000) %>%
    sample_frac(1) %>%
    filter(!(page_id %in% about_dat$page_id)) %>%
    filter(!(page_id %in% already_there)) %>%
    filter(!(page_id %in% deleted_pages$page_id)) %>%
    mutate(runni = 1:n()) %>%
    mutate(runni = runni %% 10) #%>%
# group_split(runni)
#

saveRDS(splitted, file = "data/splitted.rds")

# rstudioapi::jobRunScript("controller.R")

get_new(splitted, reconnect = F)

# # debugonce(ad_library_about)
# page_id_dat %>%
#   # slice(140001:n()) %>%
#   filter(!(page_id %in% about_dat$page_id)) %>%
#   filter(!(page_id %in% already_there)) %>%
#   filter(!(page_id %in% deleted_pages$page_id)) %>%
#   # slice(1:5000) %>%
#   # slice(5000:10000) %>%
#   # slice(10000:15000) %>%
#   # slice(15000:20000) %>%
#   # slice(20000:25000) %>%
#   # slice(25000:30000) %>%
#   # slice(35000:n()) %>%
#   # sample_n(n()) %>%
#   mutate(runni = 1:n()) %>%
#   mutate(runni = runni %% 10) %>% #count(runni) %>%
#   # group_split(runni) %>%
#   get_new(reconnect = F)
# walk(~{
# # batch_name <<- batch_name+1
# get_new(.x)
# sav
# })

stop("hi")

library(tidyverse)
readRDS_pos <- possibly(readRDS, otherwise = NULL)
about_dat <- dir("data/abouts", full.names = T) %>%
    tibble(datpath = .) %>%
    # filter(magrittr::is_in(str_remove_all(datpath, "data/abouts/|\\.rds"), al_strings)) %>%
    pull(datpath) %>%
    map_dfr_progress(readRDS_pos) %>%
    distinct()


saveRDS(about_dat, file = "data/about_dat.rds")

about_dat %>%
    mutate(chr_length = str_count(about_text)) %>%
    arrange(desc(chr_length)) %>%
    count(about_text, sort = T)

# get_new()

# library(tidyverse)
while (T) {
    cat(paste0("\r",length(dir("data/abouts", full.names = T))))
    flush.console()
}
#
# %>%
#   walk(browseURL)

ad_library_about("https://www.facebook.com/ads/library/?active_status=all&ad_type=political_and_issue_ads&country=US&view_all_page_id=7860876103&sort_data[direction]=desc&sort_data[mode]=relevancy_monthly_grouped&search_type=page&media_type=all")


str_detect("SNK", "^(?!.*Hostivice město).*\bSNK\b.*")


### FILE:  R/get_media.R  ###

#' Find an object in a nested list by name
#'
#' @param haystack A nested list.
#' @param needle Name of the object to find.
#' @return Object in the nested list with the given name or NULL if not found.
#' @export
find_name <- function(haystack, needle) {
    if (hasName(haystack, needle)) {
        haystack[[needle]]
    } else if (is.list(haystack)) {
        for (obj in haystack) {
            ret <- Recall(obj, needle)
            if (!is.null(ret)) {
                return(ret)
            }
        }
    } else {
        NULL
    }
}

#' Detect the JSON code on Facebook ad websites
#'
#' Function to detect the JSON code on facebook ad websites that contains the media URLs
#' This is basically str_extract but with perl!
#'
#' @param rawhtmlascharacter Raw HTML content as character string.
#' @return A parsed JSON object.
#' @seealso \link{detectmysnap}
detectmysnap_dep <- function(rawhtmlascharacter) {

    # Detect the position of the snapshot json entry
    detection <-
        rawhtmlascharacter %>%
        regexpr(
            text = .,
            pattern = '"snapshot":((\\w*)\\s*\\(((?:(?>[^(){}]+)|(?1))*)\\)|\\{((?:(?>[^(){}]+)|(?1))*)\\})',
            perl = T
        )

    # Extract the json that follows snapshot (without then name, 'snapshot')
    rawhtmlascharacter %>%
        stringr::str_sub(detection[1], detection[1] + attr(detection, "match.length") - 1) %>%
        stringr::str_remove('"snapshot":') %>%
        jsonlite::fromJSON() %>%
        return()
}

#' Updated function to detect the JSON code on Facebook ad websites
#'
#' @inheritParams detectmysnap_dep
#' @return A parsed JSON object.
detectmysnap <- function(rawhtmlascharacter) {
    haystackpart <-
        rawhtmlascharacter %>%
        stringr::str_split('"snapshot":') %>%
        purrr::pluck(1) %>%
        purrr::pluck(2)

    # Detect the position of the snapshot json entry
    detection <-
        haystackpart %>%
        regexpr(
            text = .,
            pattern = "\\{(?:[^}{]+|(?R))*+\\}",
            perl = T
        )

    # Extract the json that follows snapshot (without then name, 'snapshot')
    haystackpart %>%
        stringr::str_sub(detection[1], detection[1] + attr(detection, "match.length") - 1) %>%
        stringr::str_remove('"snapshot":') %>%
        jsonlite::fromJSON() %>%
        return()
}





#' Convert an entry for an ad into a row in a tibble
#'
#' We did not manage to easily output the entry for one ad as a row in a tibble; this line solves the issue and does exactly that.
#'
#' @param x An object containing data about the ad.
#' @return A tibble row.
#' @export
stupid_conversion <- function(x) {
    list("f" = x) %>%
        tibble::enframe() %>%
        tidyr::unnest_wider(value)
}
# THIS MAY BE AN ALTERNATIVE SOLUTION TO THE PROBLEM
# x <- tibble(data = page_one_content$data)
# df_imp <- x %>%
#   unnest_wider(data)


#' Get ad snapshots from Facebook ad library
#'
#' @param ad_id A character string specifying the ad ID.
#' @param download Logical, whether to download media files.
#' @param mediadir Directory to save media files.
#' @param hashing Logical, whether to hash the files. RECOMMENDED!
#' @return A tibble with ad details.
#' @export
get_ad_snapshots <- function(ad_id, download = F, mediadir = "data/media", hashing = F) {

    html_raw <- rvest::read_html(glue::glue("https://www.facebook.com/ads/library/?id={ad_id}"))
    script_seg <- html_raw %>%
        rvest::html_nodes("script") %>%
        as.character() %>%
        .[stringr::str_detect(., "snapshot")]
    dataasjson <- detectmysnap(script_seg)
    fin <- dataasjson %>%
        stupid_conversion() %>%
        dplyr::mutate(id = ad_id)

    if (download) {
        # try({
        fin %>% download_media(mediadir = mediadir,
                               hashing)
        # })
    }

    return(fin)
}



#' Apply a function to each element of a list with a progress bar
#'
#' Adding a progress bar to the map_dfr function https://www.jamesatkins.net/posts/progress-bar-in-purrr-map-df/
#'
#' @param .x List to iterate over.
#' @param .f Function to apply.
#' @param ... Other parameters passed to \code{purrr::map_dfr}.
#' @param .id An identifier.
#' @return An aggregated data frame.
#' @export
map_dfr_progress <- function(.x, .f, ..., .id = NULL) {
    .f <- purrr::as_mapper(.f, ...)
    pb <- progress::progress_bar$new(
        total = length(.x),
        format = " (:spin) [:bar] :percent | :current / :total | elapsed: :elapsedfull | eta: :eta",
        # format = " downloading [:bar] :percent eta: :eta",
        force = TRUE)

    f <- function(...) {
        pb$tick()
        .f(...)
    }
    purrr::map_dfr(.x, f, ..., .id = .id)
}

#' Walk through a list with a progress bar
#'
#' @inheritParams map_dfr_progress
#' @return None (used for side effects).
#' @export
walk_progress <- function(.x, .f, ..., .id = NULL) {
    .f <- purrr::as_mapper(.f, ...)
    pb <- progress::progress_bar$new(
        total = length(.x),
        format = " (:spin) [:bar] :percent | :current / :total | elapsed: :elapsedfull | eta: :eta",
        # format = " downloading [:bar] :percent eta: :eta",
        force = TRUE)

    f <- function(...) {
        pb$tick()
        .f(...)
    }
    purrr::walk(.x, f, ..., .id = .id)
}


#' Extract media URLs from data
#'
#' @param yo Data containing potential media URLs.
#' @return A character vector of media URLs.
extract_media_urls <- function(yo) {
    yo <- yo
    # print(yo$id)

    if (any(!is.na(yo$images))) {
        # print("image")
        temp <- unlist(yo$images)
        dl_links <- temp[stringr::str_detect(names(temp), "original_image_url")]
        # dl_links <- unlist(yo$images)[["original_image_url"]]
    } else if (any(!is.na(yo$videos))) {
        # print("videos")
        dl_links <- unlist(yo$videos)[["video_hd_url"]]
    } else if (any(!is.na(yo$cards))) {
        # print("cards")
        raw_cards <- unlist(yo$cards) # [["original_image_url"]]

        vi <- raw_cards %>%
            .[stringr::str_detect(names(.), "video_hd")]

        im <- raw_cards %>%
            .[stringr::str_detect(names(.), "original_image")]

        if (!all(is.na(vi))) {
            # print("videos")
            ## sometimes every single vi is the same so only keep unique
            dl_links <- vi %>% unique()
        }

        if (!all(is.na(im))) {
            # print("images")
            ## maybe its possible that there vi and im? then combine!
            if (exists("dl_links")) {

                ## sometimes every single im is the same so only keep unique
                dl_links <- im %>%
                    c(dl_links) %>%
                    unique()
            } else {
                dl_links <- im %>% unique()
            }
        }
    } else {
        dl_links <- ""
    }

    dl_links <- dl_links %>%
        na.omit() %>%
        purrr::discard(~ magrittr::equals(.x, ""))

    return(dl_links)
}

#' Download media files with specified IDs
#'
#' @param id Ad ID.
#' @param x Media URLs to download.
#' @param n Number of URLs.
#' @param mediadir Directory to save media files.
#' @return A character vector with file paths.
download_media_int <- function(id, x, n, mediadir = "data/media") {
    if (n > 1) {
        counter <- 0
    }

    # stage all downloaded media files in a waiting room folder
    thefiles <-
        x %>%
        purrr::map_chr(~ {
            fol <- "waiting_room"
            if (stringr::str_detect(.x, "video")) {
                ending <- "mp4"
                # fol <- "vid_hash"
            } else if (stringr::str_detect(.x, "jpg")) {
                ending <- "jpg"
                # fol <- "img_hash"
            }

            if (n > 1) {
                counter <<- counter + 1
                sub <- glue::glue("_{counter}")
            } else {
                sub <- ""
            }

            thefile <- glue::glue("{mediadir}/{fol}/{id}{sub}.{ending}")

            download.file(
                quiet = T,
                url = .x, mode = "wb",
                destfile = thefile
            )

            return(thefile)
        })
    return(thefiles)
}

# DEPRECATED
download_media_int_dep <- function(id, x, n, mediadir = "data/media") {
    if (n > 1) {
        counter <- 0
    }

    x %>%
        purrr::walk(~ {
            if (stringr::str_detect(.x, "video")) {
                ending <- "mp4"
                fol <- "vid_hash"
            } else if (stringr::str_detect(.x, "jpg")) {
                ending <- "jpg"
                fol <- "img_hash"
            }

            if (n > 1) {
                counter <<- counter + 1
                sub <- glue::glue("_{counter}")
            } else {
                sub <- ""
            }

            download.file(
                url = .x, mode = "wb",
                destfile = glue::glue("{mediadir}/{fol}/{id}{sub}.{ending}")
            )
        })
}

safe_copy <- function(yoooyyy, whereto) {
    file.copy(yoooyyy, whereto)
    return(whereto)
}

# safe_img_read <- possibly(OpenImageR::readImage, otherwise = NULL, quiet = T)

#' Check hash of a media file
#'
#' @param .x Path to the media file.
#' @param hash_table A data frame of existing hashes.
#' @param mediadir Directory to save media files.
#' @return A tibble with file details.
check_hash <- function(.x, hash_table, mediadir){
    if (stringr::str_detect(.x, "mp4")) {
        # print("video")
        hashy <- digest::digest(
            object = .x,
            file = T,
            algo = "md5",
        )
        type <- "vid"
        ending <- "mp4"
    } else if (stringr::str_detect(.x, "jpg")) {
        # print("image")
        img_to_read_in <<- .x

        ending <- get_file_ending(img_to_read_in) %>% stringr::str_split("/") %>% unlist() %>% dplyr::first()

        ## it detects as jpeg all of a sudden?
        if(!ending %in% c("jpeg", "jpg")){ # if the image ending is anything other than jpg|jpeg, then correct it
            file.rename(img_to_read_in, str_replace(img_to_read_in, "jpg", ending))
            img_to_read_in <- str_replace(img_to_read_in, "jpg", ending)
            # accordingly correct the dataframe
        } else {
            ending <- "jpg"
        }

        hashy <- digest::digest(
            object = img_to_read_in,
            file = T,
            algo = "md5",
        )

        type <- "img"
    }

    if(type == "img") {
        path <- img_to_read_in
    } else if (type == "vid"){
        path <- .x
    }

    # print(hashy)
    hash_table_row <- tibble::tibble(
        hash = hashy,
        ad_id = .x %>%
            stringr::str_split("/") %>%
            unlist() %>% dplyr::last() %>%
            stringr::str_remove_all(".jpg|.mp4") %>%
            paste0("adid_", .),
        media_type = type,
        ending,
        filepath = path
    )


    return(hash_table_row)
}


#' Create necessary directories
#'
#' @param x Directory path to check and create.
#' @return None (used for side effects).
create_necessary_dirs <- function(x) {
    if(!file.exists(x)){
        dir.create(x, recursive = T)
    }
}

#' Download media files and potentially hash them
#'
#' @param media_dat Data containing media URLs.
#' @param mediadir Directory to save media files.
#' @param hashing Logical, whether to hash the files.
#' @return None (used for side effects).
#' @export
download_media <- function(media_dat,
                           mediadir = "data/media",
                           hashing = T) {

    c(glue::glue("{mediadir}"),
      glue::glue("{mediadir}/img_hash"),
      glue::glue("{mediadir}/vid_hash"),
      glue::glue("{mediadir}/waiting_room")) %>%
        purrr::walk(create_necessary_dirs)

    if (hashing) {

        hash_table_path <- glue::glue("{mediadir}/hash_table.csv")
        # Check if Hash table file is already existing
        firsthash <- !file.exists(hash_table_path)

        if (firsthash) {
            hash_table <- tibble::tibble(
                hash = NA_character_,
                ad_id = NA_character_,
                media_type = NA_character_,
                ending = NA_character_,
                filepath = NA_character_
            )
            # write_csv(hash_table_row,file = glue("{datadir}/media/hash_table.csv"))
        } else {
            hash_table <- readr::read_csv(hash_table_path, col_types = readr::cols(.default = readr::col_character()))
        }

        # hash_nrow <- nrow(hash_table)
    }

    # take media urls out of media_dat
    the_urls <- media_dat %>%
        extract_media_urls()

    # if at least 1 URL, then start downloading and hashing
    if (length(the_urls) != 0) {
        thefiles <<- download_media_int(media_dat$id, the_urls, length(the_urls), mediadir = mediadir)

        # print(glue::glue("Found {length(thefiles)} media files to download."))

        # Hashing of all media
        if (hashing) {

            unique_counter <<- 0

            hash_table_rows <<- thefiles %>%
                purrr::map_dfr(
                    ~ {check_hash(.x, hash_table, mediadir)}
                )

            # ex <<- hash_table_row %>%
            #   bind_rows(hash_table_row[1,] %>% mutate(ad_id = "yoyo"))

            ### check if hashes are already present and if so copy paste to folder
            hash_table_rows %>%
                dplyr::filter(!(hash %in% hash_table$hash)) %>%
                split(1:nrow(.)) %>%
                purrr::walk(~{
                    filter_again <- .x %>%
                        dplyr::filter(!(hash %in% hash_table$hash))
                    if(nrow(filter_again)!=0){
                        file.copy(from = .x$filepath, to = glue::glue("{mediadir}/{.x$media_type}_hash/{.x$hash}.{.x$ending}"))
                        unique_counter <<- unique_counter + 1
                    }

                    hash_table <<- hash_table %>% dplyr::bind_rows(.x)

                })

            # print(glue::glue("Copied {unique_counter} unique media files."))

            # empty waiting rooms
            dir(glue::glue("{mediadir}/waiting_room"), full.names = T) %>% purrr::walk(file.remove)

            # print(hash_table)
            if(firsthash) {
                readr::write_csv(hash_table %>% tidyr::drop_na() %>% dplyr::distinct(.keep_all = T), hash_table_path)
            } else {
                readr::write_csv(hash_table_rows %>% dplyr::distinct(.keep_all = T), hash_table_path, append = T)
            }

        }
    }
}

# Read file endings from r object
get_file_ending <- function(file_full_path) {

    file_mime_type <- system2(command = "file",
                              args = paste0(" -b --mime-type ", file_full_path), stdout = TRUE) # "text/rtf"
    # Gives the list of potentially allowed extension for this mime type:
    file_possible_ext <- system2(command = "file",
                                 args = paste0(" -b --extension ", file_full_path),
                                 stdout = TRUE) # "???". "doc/dot" for MsWord files.

    return(file_possible_ext)

}

# datadir <- "data"
#
# library(glue)
# library(rvest)
# library(jsonlite)

# debug section
if(F){
    debugonce(detectmysnap)
    debugonce(get_ad_snapshots)
    get_ad_snapshots("561403598962843", download = T, hashing = T, mediadir = glue("{datadir}/media"))
    ad_id <- "1103135646905363"
    browseURL(glue("https://www.facebook.com/ads/library/?id={ad_id}")) # to browse the specific ad
}
#



#' Extract and flatten 'deeplinkAdCard' JSON from a Facebook Ad Library page
#'
#' This function programmatically retrieves the embedded JSON object labeled `deeplinkAdCard`
#' from the source code of a Facebook Ad Library ad page.
#'
#' The function performs the following steps internally:
#'
#' 1. Fetches the ad page HTML from Facebook's Ad Library for the specified `ad_id`.
#' 2. Locates the `<script>` tag containing the `deeplinkAdCard` object.
#' 3. Uses a recursive regular expression to extract the full JSON object following `deeplinkAdCard`.
#' 4. Parses the JSON string into a nested R list.
#' 5. Flattens the JSON into a tidy tibble row, unnesting nested sub-objects such as `fevInfo`,
#'    `free_form_additional_info`, `learn_more_content`, and optionally `snapshot` if present.
#'
#' The output is designed for downstream analysis: each ad is represented as **one row** in a tibble,
#' with nested JSON fields expanded into their own columns via `tidyr::unnest_wider()`.
#'
#' This function complements `get_ad_snapshots()`, which extracts the `snapshot` JSON.
#' Use `get_deeplink()` when additional metadata embedded under `deeplinkAdCard` is required.
#'
#' @param ad_id Character string specifying the Facebook ad ID (as shown in the Ad Library URL).
#' @return A tibble with one row, containing flattened columns extracted from the deeplink JSON object.
#' Columns depend on the structure of the JSON and may include fields like `fevInfo_*`,
#' `fevInfo_free_form_additional_info_*`, `fevInfo_learn_more_content_*`, and snapshot-related columns.
#'
#' @seealso [get_ad_snapshots()] for extracting snapshot JSON; [detectmysnap()] for raw JSON detection.
#'
#' @examples
#' \dontrun{
#' df <- get_deeplink("1103135646905363")
#' glimpse(df)
#' }
#'
#' @export
get_deeplink <- function(ad_id) {

  html_raw <- rvest::read_html(glue::glue("https://www.facebook.com/ads/library/?id={ad_id}"))

  script_seg <- html_raw %>% rvest::html_nodes("script") %>%
    as.character() %>% .[stringr::str_detect(., "snapshot")]


  # 1. Extract the part after "deeplinkAdCard":
  haystackpart <- script_seg %>%
    stringr::str_split('"deeplinkAdCard":') %>%
    purrr::pluck(1, 2)  # f

  if (is.null(haystackpart)) {
    stop("No 'deeplinkAdCard' found in input.")
  }

  # 2. Detect the JSON object { ... } using recursive regex
  detection <- regexpr(
    text = haystackpart,
    pattern = "\\{(?:[^}{]+|(?R))*+\\}",
    perl = TRUE
  )

  if (detection[1] == -1) {
    stop("Could not parse JSON object after 'deeplinkAdCard'")
  }

  # 3. Extract JSON substring
  json_string <- haystackpart %>%
    stringr::str_sub(detection[1], detection[1] + attr(detection, "match.length") - 1)

  # 4. Parse JSON into list
  parsed_json <- jsonlite::fromJSON(json_string)

  # 5. Start flattening
  df <- tibble::tibble(data = list(parsed_json)) %>%
    tidyr::unnest_wider(data)

  # 6. Conditional unnesting
  for (col in c("fevInfo", "fevInfo_free_form_additional_info", "fevInfo_learn_more_content")) {
    if (col %in% names(df)) {
      df <- tidyr::unnest_wider(df, {{col}}, names_sep = "_")
    }
  }

  # 7. Flatten snapshot (if exists)
  if ("snapshot" %in% names(parsed_json)) {
    snapshot_flat <- tibble::tibble(snapshot = list(parsed_json$snapshot)) %>%
      tidyr::unnest_wider(snapshot, names_sep = "_")

    df <- dplyr::bind_cols(df, snapshot_flat)
  }

  return(df)
}


### FILE:  R/get_page_insights.R  ###

#' Get Page Insights
#'
#' Retrieves insights for a given Facebook page within a specified timeframe, language, and country.
#' It allows for fetching specific types of information and optionally joining page info with targeting info.
#'
#' @param pageid A string specifying the unique identifier of the Facebook page.
#' @param timeframe A string indicating the timeframe for the insights. Valid options include predefined
#'        timeframes such as "LAST_30_DAYS". The default value is "LAST_30_DAYS".
#' @param lang A string representing the language locale to use for the request, formatted as language
#'        code followed by country code (e.g., "en-GB" for English, United Kingdom). The default is "en-GB".
#' @param iso2c A string specifying the ISO-3166-1 alpha-2 country code for which insights are requested.
#'        The default is "US".
#' @param include_info A character vector specifying the types of information to include in the output.
#'        Possible values are "page_info" and "targeting_info". By default, both types of information are included.
#' @param join_info A logical value indicating whether to join page info and targeting info into a single
#'        data frame (if TRUE) or return them as separate elements in a list (if FALSE). The default is TRUE.
#'
#' @return If \code{join_info} is TRUE, returns a data frame combining page and targeting information for
#'         the specified Facebook page. If \code{join_info} is FALSE, returns a list with two elements:
#'         \code{page_info} and \code{targeting_info}, each containing the respective data as a data frame.
#'         In case of errors or no data available, the function may return a simplified data frame or list
#'         indicating the absence of data.
#'
#' @examples
#' insights <- get_page_insights(pageid="123456789", timeframe="LAST_30_DAYS", lang="en-GB", iso2c="US",
#'                               include_info=c("page_info", "targeting_info"), join_info=TRUE)
#'
#' @export
#'
#' @importFrom httr2 request req_headers req_body_raw req_perform
#' @importFrom jsonlite fromJSON
#' @importFrom rvest html_element html_text
#' @importFrom dplyr mutate_all select bind_cols left_join slice
#' @importFrom purrr set_names flatten discard imap_dfr is_empty
#' @importFrom stringr str_split str_remove
#' @importFrom tibble as_tibble
get_page_insights <- function(pageid, timeframe = "LAST_30_DAYS", lang = "en-GB", iso2c = "US", include_info = c("page_info", "targeting_info"), join_info = T) {

  if(timeframe %in% c(7, "7")){
    timeframe <- "LAST_7_DAYS"
  } else if(timeframe %in% c(30, "30")){
    timeframe <- "LAST_30_DAYS"
  } else if(timeframe %in% c(90, "90")){
    timeframe <- "LAST_90_DAYS"
  }

  # Randomize the user agent
ua_list <- c(
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36"
)

if("page_info" %in% include_info ){
  include_page_info <- T
} else {
  include_page_info <- F

}

ua <- sample(ua_list, 1)
# print(ua)

# Define static parameters
static_params <- list(
  av = "0",                        # Likely application version; may not change often. Optional.
  "_aaid" = "0",                   # Anonymous Advertising ID; unique to the visitor. Required for tracking purposes.
  user = "0",                      # User identifier; likely session-based or placeholder. Required.
  a = "1",                         # Arbitrary request parameter; purpose unclear but likely required.
  req = "3",                       # Request parameter; often a sequence or batch request identifier. Likely required.
  hs = "19797.BP%3ADEFAULT.2.0..0.0", # Host session or configuration metadata; required for server-side routing.
  ccg = "EXCELLENT",               # Connection grade; describes network quality. Optional but useful for server-side optimizations.
  csr = "",                        # CSRF token; placeholder here, likely required in some contexts.
  `_jssesw` = "1",                 # Encoded session value. Required for session management.
  fb_api_caller_class = "RelayModern", # API metadata describing the client. Required.
  fb_api_req_friendly_name = "AdLibraryMobileFocusedStateProviderQuery", # API-friendly name for request logging. Optional.
  server_timestamps = "true",      # Flag indicating server timestamps should be included. Likely required.
  doc_id = "7193625857423421"      # Unique document ID for the query. Required for identifying the request schema.
)

# Construct variables
variables <- jsonlite::toJSON(
  list(
    adType = "POLITICAL_AND_ISSUE_ADS",      # Type of ads to query. Required.
    audienceTimeframe = timeframe,          # Timeframe for audience data (e.g., LAST_30_DAYS). Required.
    country = iso2c,                        # Country ISO code (e.g., "DE"). Required.
    viewAllPageID = pageid,                 # Page ID for which data is being queried. Required.
    fetchPageInfo = include_page_info,      # Boolean flag to fetch page-specific information. Optional.
    fetchSharedDisclaimers = TRUE,          # Boolean flag to include shared disclaimers. Optional but useful.
    active_status = "ALL",                  # Filter for active/inactive ads. Required.
    ad_type = "POLITICAL_AND_ISSUE_ADS",    # Type of ads (repeated for clarity). Required.
    bylines = list(),                       # List of bylines to filter ads. Optional.
    content_languages = list(),             # Filter for content languages. Optional.
    count = 30,                             # Number of results to fetch. Optional but usually required for pagination.
    countries = list(iso2c),                # List of countries for filtering (repeated for clarity). Required.
    excluded_ids = list(),                  # IDs to exclude from results. Optional.
    full_text_search_field = "ALL",         # Full-text search field filter. Optional.
    group_by_modes = list(),                # Grouping modes for results. Optional.
    search_type = "PAGE",                   # Type of search (e.g., by page). Required.
    sort_data = list(
      mode = "SORT_BY_RELEVANCY_MONTHLY_GROUPED", # Sorting mode. Required.
      direction = "ASCENDING"                    # Sorting direction. Required.
    )
  ),
  auto_unbox = TRUE
)

static_params$variables <- URLencode(variables)
body <- paste(names(static_params), static_params, sep = "=", collapse = "&")

# print("Constructed body:")
# print(body)

resp <- httr2::request("https://www.facebook.com/api/graphql/") %>%
  httr2::req_headers(
    `Accept-Language` = paste0(
      lang, ",", stringr::str_split(lang, "-") %>% unlist() %>% .[1], ";q=0.5"
    ),
    `sec-fetch-site` = "same-origin",
    `user-agent` = ua
  ) %>%
  httr2::req_body_raw(body, "application/x-www-form-urlencoded") %>%
  httr2::req_perform()

out <- resp %>%
  httr2::resp_body_html() %>%
  rvest::html_element("p") %>%
  rvest::html_text() %>%
  stringr::str_split_1('(?<=\\})\\s*(?=\\{)') %>%
  purrr::map(jsonlite::fromJSON)

if(!is.null(out[[1]][["errors"]][["description"]])){
  message(out[[1]][["errors"]][["description"]])
}

if( "page_info" %in% include_info) {
  page_info1 <-
    out[[1]][["data"]][["ad_library_page_info"]][["page_info"]]

  if(is.null(page_info1)){

    if ("page_info" %in% include_info & "targeting_info" %in% include_info) {
      if (join_info) {
        return(tibble::tibble(page_id = pageid, no_data = T))
      } else {
        return(list(page_info = tibble::tibble(page_id = pageid, no_data = T),
                    targeting_info = tibble::tibble(page_id = pageid, no_data = T)))
      }

    } else {
      return(tibble::tibble(page_id = pageid, no_data = T))
    }



  }

  my_dataframe <-
    as.data.frame(t(unlist(page_info1)), stringsAsFactors = FALSE) %>%
    dplyr::mutate_all(as.character)

  page_info2_raw <-
    out[[2]][["data"]][["page"]][["shared_disclaimer_info"]][["shared_disclaimer_pages"]][["page_info"]]
  if (!is.null(page_info2_raw)) {
    page_info2 <- page_info2_raw   %>%
      tibble::as_tibble() %>%
      dplyr::mutate_all(as.character) %>%
      dplyr::mutate(shared_disclaimer_info = my_dataframe$page_id[1])
  } else {
    page_info2 <- tibble::tibble(no_shared_disclaimer  = T)
  }



    creat_times <- out[[1]][["data"]][["page"]][["pages_transparency_info"]][["history_items"]]

    if (!is.null(creat_times)) {
      creat_times <- creat_times %>%  dplyr::mutate(event = paste0(item_type, ": ", as.POSIXct(event_time,
                                                                                                origin = "1970-01-01", tz = "UTC"))) %>% dplyr::select(event) %>%
        unlist() %>% t() %>% as.data.frame()
    }  else {
      creat_times <- tibble(no_times = T)
    }

    about_text <- out[[1]][["data"]][["page"]][["about"]]

    if (!is.null(about_text)) {
      about_text <- about_text  %>%
        purrr::set_names("about")
    } else {
      about_text <- tibble(no_about = T)
    }

  address_raw <- out[[1]][["data"]][["page"]][["confirmed_page_owner"]][["information"]]
  if(!is.null(address_raw)){
      address <- address_raw %>% purrr::flatten()
  } else {
    address <- tibble::tibble(no_address  = T)
  }


  sdis_raw <-  out[[2]][["data"]][["page"]][["shared_disclaimer_info"]][["shared_disclaimer_pages"]][["page_info"]]
  if(!is.null(sdis_raw)){
      sdis <-    sdis_raw %>%
    dplyr::mutate_all(as.character) %>%
    dplyr::mutate(shared_disclaimer_page_id = pageid[1]) %>%
    jsonlite::toJSON() %>%
    as.character()
  } else {
    sdis <- "[]"
  }

  page_info <- my_dataframe %>%
    dplyr::mutate(shared_disclaimer_info = sdis) %>%
    dplyr::bind_cols(about_text) %>%
    dplyr::bind_cols(creat_times) %>%
    dplyr::bind_cols(address)

}


if("targeting_info" %in% include_info ) {
  out_raw <-
    out[[1]][["data"]][["page"]][["ad_library_page_targeting_insight"]]

  summary_dat <- out_raw %>%
    purrr::pluck("ad_library_page_targeting_summary") %>%
    dplyr::bind_rows()

  if (nrow(summary_dat) > 1) {
    summary_dat <- summary_dat %>%
      dplyr::slice(which(
        summary_dat$detailed_spend$currency == summary_dat$main_currency
      )) %>%
      dplyr::select(-detailed_spend)

  }

  targeting_details_raw <-
    out_raw[!(
      names(out_raw) %in% c(
        "ad_library_page_targeting_summary",
        "ad_library_page_has_siep_ads"
      )
    )]

  # names(targeting_details_raw)

  targeting_info <- targeting_details_raw %>%
    purrr::discard(purrr::is_empty) %>%
    purrr::imap_dfr( ~ {
      .x %>% dplyr::mutate(type = .y %>% stringr::str_remove("ad_library_page_targeting_"))
    }) %>%
    dplyr::bind_cols(summary_dat) %>%
    dplyr::mutate(page_id = pageid)


}


if( "page_info" %in% include_info & "targeting_info" %in% include_info  ) {

  if(join_info){
    fin <- page_info %>%
      dplyr::left_join(targeting_info, by = "page_id")
  } else {
    fin <- list(page_info, targeting_info)
  }

} else if ("page_info" %in% include_info) {
  return(page_info)
}  else if ("targeting_info" %in% include_info) {
  return(targeting_info)
}

return(fin)


}


#' Get Page Info Dataset for a Specific Country
#'
#' Downloads the historical Facebook or Instagram page info dataset for a given ISO2C country code.
#' The data is retrieved from a fixed GitHub release URL in `.parquet` format. It includes information on:
#' - Page-level metadata (e.g., name, verification status, profile type)
#' - Audience metrics (e.g., number of likes, Instagram followers)
#' - Shared disclaimers (if applicable)
#' - Page creation and name change events with timestamps
#' - Contact and address information (if available)
#' - Free-text descriptions ("about" section)
#'
#'
#' @param iso2c A string specifying the ISO-3166-1 alpha-2 country code (e.g., "DE", "FR", "US").
#' @param verbose Logical. If TRUE (default), prints a status message when downloading.
#'
#' @return A tibble containing Facebook page info for the specified country.
#'         If the dataset is not available or cannot be retrieved, a tibble with \code{no_data = TRUE}
#'         and the given \code{iso2c} code is returned.
#'
#' @examples
#' \dontrun{
#'   de_info <- get_page_info_db("DE")
#'   fr_info <- get_page_info_db("FR")
#' }
#'
#' @export
#'
#' @importFrom arrow read_parquet
#' @importFrom tibble tibble
#' @importFrom dplyr mutate
get_page_info_db <- function(iso2c, verbose = TRUE) {
  # Validate input
  if (missing(iso2c) || !is.character(iso2c) || nchar(iso2c) != 2) {
    stop("Please provide a valid ISO2C country code, e.g., 'DE', 'FR', 'US'.")
  }

  # Construct URL
  url <- sprintf(
    "https://github.com/favstats/meta_ad_targeting/releases/download/MetaPageInfos/%s-page_info.parquet",
    iso2c
  )

  # Try to read
  if (verbose) message("Downloading: ", url)

  tryCatch({
    df <- arrow::read_parquet(url)
    dplyr::mutate(df, country_code = iso2c)
  },
  error = function(e) {
    if (verbose) message("No data found for ", iso2c, ". Returning placeholder.")
    tibble::tibble(country_code = iso2c, no_data = TRUE)
  })
}





#' Get Page Info Dataset for a Specific Country
#'
#' Downloads the historical Facebook or Instagram page info dataset for a given ISO2C country code.
#' The data is retrieved from a fixed GitHub release URL in `.parquet` format. It includes information on:
#' - Page-level metadata (e.g., name, verification status, profile type)
#' - Audience metrics (e.g., number of likes, Instagram followers)
#' - Shared disclaimers (if applicable)
#' - Page creation and name change events with timestamps
#' - Contact and address information (if available)
#' - Free-text descriptions ("about" section)
#'
#'
#' @param iso2c A string specifying the ISO-3166-1 alpha-2 country code (e.g., "DE", "FR", "US").
#' @param verbose Logical. If TRUE (default), prints a status message when downloading.
#'
#' @return A tibble containing Facebook page info for the specified country.
#'         If the dataset is not available or cannot be retrieved, a tibble with \code{no_data = TRUE}
#'         and the given \code{iso2c} code is returned.
#'
#' @examples
#' \dontrun{
#'   de_info <- get_page_info_db("DE")
#'   fr_info <- get_page_info_db("FR")
#' }
#'
#' @export
#'
#' @importFrom arrow read_parquet
#' @importFrom tibble tibble
#' @importFrom dplyr mutate
get_additional_page_info_db <- function(iso2c, verbose = TRUE) {
  # Validate input
  if (missing(iso2c) || !is.character(iso2c) || nchar(iso2c) != 2) {
    stop("Please provide a valid ISO2C country code, e.g., 'DE', 'FR', 'US'.")
  }

  # Construct URL
  url <- sprintf(
    "https://github.com/favstats/meta_ad_targeting/releases/download/AdditionalPageInfo/%s-page_info.parquet",
    iso2c
  )

  # Try to read
  if (verbose) message("Downloading: ", url)

  tryCatch({
    df <- arrow::read_parquet(url)
    dplyr::mutate(df, country_code = iso2c)
  },
  error = function(e) {
    if (verbose) message("No data found for ", iso2c, ". Returning placeholder.")
    tibble::tibble(country_code = iso2c, no_data = TRUE)
  })
}


### FILE:  R/get_targeting.R  ###

#' Get Meta Ad Library targeting data for a page
#'
#' This function retrieves data for the targeting criteria of a Facebook page for a specified timeframe.
#'
#' @param id A character string representing the Facebook page ID.
#' @param timeframe A character string representing the desired timeframe. Can either be "LAST_30_DAYS" or "LAST_7_DAYS". Defaults to "LAST_30_DAYS".
#' @param lang An ISO language code character string representing the desired language of the targeting criteria. Defaults to "en-GB" but can be "en-US" and many more.
#'
#' @return A `tibble` containing the audience data for the specified Facebook page and timeframe.
#'
#' @examples
#' \dontrun{
#' get_targeting("123456789")
#' get_targeting("987654321", "LAST_7_DAYS")
#' }
#'
#' @export
#'
get_targeting <- function(id, timeframe = "LAST_30_DAYS", lang = "en-GB", legacy = FALSE) {

  if(legacy){


      url <- "https://www.facebook.com/api/graphql/"

      heads_up <- httr::add_headers(`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0",
                                    Accept = "*/*",
                                    `Accept-Language` = paste0(lang, ',', stringr::str_split(lang, "-") %>% unlist() %>% .[1],';q=0.5'),
                                    `X-FB-Friendly-Name` = "AdLibraryPageAudienceTabQuery",
                                    `X-FB-LSD`= "AVrNiQCSUnA",
                                    `Alt-Used`= "www.facebook.com",
                                    `Sec-Fetch-Dest`= "empty",
                                    `Sec-Fetch-Mode`= "cors",
                                    `Sec-Fetch-Site`= "same-origin",
                                    # `Accept-Encoding` = "gzip, deflate, br",
                                    `Content-Type` = "application/x-www-form-urlencoded",
                                    Connection = "keep-alive"
      )


      if(timeframe == "LAST_30_DAYS"){

          # audienceTimeframe <- "%7B%22audienceTimeframe%22%3A%22LAST_30_DAYS%22%2C%22"
          da_body <- glue::glue("av=0&__user=0&__a=1&__dyn=7xeUmxa3-Q8zo5ObwKBWobVo9E4a2i5U4e1FxebzEdF8ixy7EiwvoWdwJwCwAwgU2lxS6Ehwem0nCqbwgE3awbG78b87C1xwEwgolzUO0n2US2G3i1ywa-2l0Fwwwi831wnFokwyx2cw8WfK6E5i3e4U3mxOu2S2W2K7o725U4q0HUkyE9E11EbodEGdw46wbLwiU8U6C2-&__csr=&__req=m&__hs=19237.BP%3ADEFAULT.2.0.0.0.0&dpr=1&__ccg=EXCELLENT&__rev=1006139712&__s=ll61s1%3Axn89ey%3Admpplc&__hsi=7138774996758193009&__comet_req=0&lsd=AVrNiQCSYrc&jazoest=2981&__spin_r=1006139712&__spin_b=trunk&__spin_t=1662125577&__jssesw=1&fb_api_caller_class=RelayModern&fb_api_req_friendly_name=AdLibraryPageAudienceTabQuery&variables=%7B%22audienceTimeframe%22%3A%22LAST_30_DAYS%22%2C%22viewAllPageID%22%3A%22{id}%22%7D&server_timestamps=true&doc_id=4756112137823411") %>% as.character()

      } else if (timeframe == "LAST_7_DAYS"){

          # audienceTimeframe <- "%7B%22"
          da_body <- glue::glue("av=0&__user=0&__a=1&__dyn=7xeUmxa3-Q8zo5ObwKBWobVo9E4a2i5U4e1FxebzEdF8aUuxa1ZzES2S2q2i13w9m7oqx60Vo1upEK12wcG0KEswIwuo662y11xmfz81sbzoaEd86a0HU9k2C2218wc61uBxi2a48O0zE-Uqwl8cUjwdq79UbobEaUtws8nwhE2LxiawCw46wJwSyES0gq0K-1bwzwqobU&__csr=&__req=f&__hs=19245.BP%3ADEFAULT.2.0.0.0.0&dpr=1&__ccg=EXCELLENT&__rev=1006179750&__s=njkc5w%3A6o847a%3A9gcoa8&__hsi=7141736891942848978&__comet_req=0&lsd=AVrbeuAiHJg&jazoest=21000&__spin_r=1006179750&__spin_b=trunk&__spin_t=1662815197&__jssesw=1&fb_api_caller_class=RelayModern&fb_api_req_friendly_name=AdLibraryPageAudienceTabQuery&variables=%7B%22audienceTimeframe%22%3A%22LAST_7_DAYS%22%2C%22viewAllPageID%22%3A%22{id}%22%7D&server_timestamps=true&doc_id=4756112137823411") %>% as.character()

      } else if (timeframe == "LAST_90_DAYS"){

          da_body <- glue::glue("av=0&__user=0&__a=1&__dyn=7xeUmxa3-Q8zo5ObwKBWobVo9E4a2i5U4e1FxebzEdF8aUuxa1ZzES2S2q2i13w9m7oqx60Vo1upEK12wcG0KEswIwuo662y11xmfz81sbzoaEd86a0HU9k2C2218wc61uBxi2a48O3u1mzXxG1kwPxe3C0D8sDwJwKwHxS1Mxu16wa-58G2q0gq2S3qazo11E2XU4K2e1FwLw8O2i&__csr=&__req=h&__hs=19301.BP%3ADEFAULT.2.0.0.0.0&dpr=1&__ccg=EXCELLENT&__rev=1006553893&__s=20shv5%3A62a2bj%3A6goj90&__hsi=7162612241770415577&__comet_req=0&lsd=AVohzhTn68E&jazoest=2965&__spin_r=1006553893&__spin_b=trunk&__spin_t=1667675618&__jssesw=1&fb_api_caller_class=RelayModern&fb_api_req_friendly_name=AdLibraryPageAudienceTabQuery&variables=%7B%22audienceTimeframe%22%3A%22LAST_90_DAYS%22%2C%22viewAllPageID%22%3A%22{id}%22%7D&server_timestamps=true&doc_id=4756112137823411") %>% as.character()

          url <- "https://www.facebook.com/api/graphql/"

          heads_up <- httr::add_headers(`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0",
                                        Accept = "*/*",
                                        `Accept-Language` = paste0(lang, ',', stringr::str_split(lang, "-") %>% unlist() %>% .[1],';q=0.5'),
                                        `X-FB-Friendly-Name` = "AdLibraryPageAudienceTabQuery",
                                        `X-FB-LSD`= "AVrNiQCSUnA",
                                        `Alt-Used`= "www.facebook.com",
                                        `Sec-Fetch-Dest`= "empty",
                                        `Sec-Fetch-Mode`= "cors",
                                        `Sec-Fetch-Site`= "same-origin",
                                        # `Accept-Encoding` = "gzip, deflate, br",
                                        `Content-Type` = "application/x-www-form-urlencoded",
                                        Connection = "keep-alive"
          )

      }





      posted = httr::POST(url, heads_up, body = da_body)

      contentwise <- httr::content(posted)

      rate_limit <- stringr::str_detect(as.character(contentwise), "Rate limit exceeded")
      if(rate_limit){
          stop(as.character(contentwise))
      }



      out_raw <- contentwise %>%
          rvest::html_nodes("body") %>%
          rvest::html_nodes("p") %>%
          as.character() %>% stringr::str_remove_all("</p>|<p>") %>%
          jsonlite::fromJSON()  %>%
          purrr::pluck("data") %>%
          purrr::pluck("page") %>%
          purrr::pluck("ad_library_page_targeting_insight")


      summary_dat <- out_raw %>%
          purrr::pluck("ad_library_page_targeting_summary") %>%
          dplyr::bind_rows()

      if(nrow(summary_dat) > 1){

          summary_dat <- summary_dat %>%
              dplyr::slice(which(summary_dat$detailed_spend$currency == summary_dat$main_currency)) %>%
              dplyr::select(-detailed_spend)

      }

      targeting_details_raw <- out_raw[!(names(out_raw) %in% c("ad_library_page_targeting_summary", "ad_library_page_has_siep_ads"))]

      # names(targeting_details_raw)

      res <- targeting_details_raw %>%
          purrr::discard(purrr::is_empty) %>%
          purrr::imap_dfr(~{.x %>% dplyr::mutate(type = .y %>% stringr::str_remove("ad_library_page_targeting_"))}) %>%
          dplyr::bind_cols(summary_dat) %>%
          dplyr::mutate(internal_id = id)



  } else {
      res <- get_page_insights(id, timeframe = timeframe,lang = lang, include_info = "targeting_info")

  }



  return(res)

}


#' Aggregate a Pre-Combined Targeting Dataset
#'
#' This function takes a single dataframe, assumed to be the result of
#' `bind_rows()` on multiple targeting datasets from different time periods.
#' It correctly aggregates the spending for each unique targeting criterion
#' and calculates the new totals and percentages based on the combined data.
#'
#' @param combined_df A single dataframe that has already been combined from
#'   multiple sources (e.g., via `dplyr::bind_rows`).
#' @param filter_disclaimer An optional character vector of disclaimers or page
#'   names to filter the dataset before aggregation. If NULL (default), all
#'   data is used.
#'
#' @return A single, aggregated tibble where each row represents a unique targeting
#'   criterion for an advertiser across the combined period.
#'
#' @import dplyr
#' @import readr
#'
aggr_targeting <- function(combined_df) {

  # --- 1. Prepare the Combined Dataset ---

  # Ensure all relevant columns are parsed as numbers for calculations
  prepared_data <- combined_df %>%
    dplyr::mutate(
      across(c(total_spend_pct, total_spend_formatted, num_ads, total_num_ads),
             ~readr::parse_number(as.character(.)))
    )

  # --- 2. Correctly Calculate Overall Page Totals ---

  # The key to fixing the error is to calculate the true total spend for each
  # advertiser across ALL underlying datasets. The `ds` column (dataset source)
  # is crucial for this.
  page_overall_totals <- prepared_data %>%
    # First, find the unique total spend for each page within each source dataset
    dplyr::distinct(page_id, ds, .keep_all = TRUE) %>%
    # Now, sum these unique totals to get the true overall spend
    dplyr::group_by(page_id) %>%
    dplyr::summarise(
      total_spend = sum(total_spend_formatted, na.rm = TRUE),
      total_num_ads = sum(total_num_ads, na.rm = TRUE),
      .groups = "drop"
    )

  # --- 3. Aggregate Spending by Unique Targeting Criterion ---

  targeting_group_vars <- c(
    "page_id", "page_name", "disclaimer", "value", "type", "location_type",
    "main_currency", "is_exclusion", "detailed_type", "num_obfuscated",
    "custom_audience_type", "cntry"
  )

  aggregated_criteria <- prepared_data %>%
    # Calculate the absolute spend for each individual criterion row
    dplyr::mutate(criterion_spend = total_spend_formatted * total_spend_pct) %>%
    dplyr::group_by(dplyr::across(dplyr::all_of(targeting_group_vars))) %>%
    dplyr::summarise(
      # Sum the spend and ad counts for each unique targeting group
      spend_per = sum(criterion_spend, na.rm = TRUE),
      num_ads = sum(num_ads, na.rm = TRUE),
      .groups = "drop"
    )

  # --- 4. Final Join and Recalculation ---

  final_data <- aggregated_criteria %>%
    # Join the correct overall totals back to the aggregated criteria data
    dplyr::left_join(page_overall_totals, by = "page_id") %>%
    # Recalculate the percentage based on the new, correct total spend
    dplyr::mutate(
      total_spend_pct = ifelse(total_spend > 0, spend_per / total_spend, 0)
    ) %>%
    # Add the internal_id and select/order columns for the final output
    dplyr::mutate(internal_id = as.character(page_id)) %>%
    dplyr::select(
      page_id, page_name, disclaimer, value, type, location_type, main_currency,
      is_exclusion, detailed_type, num_obfuscated, custom_audience_type, cntry,
      spend_per, total_spend, num_ads, total_num_ads, total_spend_pct, internal_id, everything()
    )

  return(final_data)
}


### FILE:  R/ggl.R  ###

#' Retrieve Google Ad Spending Data
#'
#' This function queries the Google Ad Library to retrieve information about
#' advertising spending for a specified advertiser. It supports a range of countries
#' and can return either aggregated data or time-based spending data.
#'
#' @param advertiser_id A string representing the unique identifier of the advertiser.
#'   For example "AR14716708051084115969".
#' @param start_date An integer representing the start date for data retrieval
#'   in YYYYMMDD format.  For example 20231029.
#' @param end_date An integer representing the end date for data retrieval
#'   in YYYYMMDD format. For example 20231128.
#' @param cntry A string representing the country code for which the data is to be retrieved.
#'   For example "NL" (Netherlands).
#' @param get_times A boolean indicating whether to return time-based spending data.
#'   If FALSE, returns aggregated data. Default is FALSE.
#'
#' @return A tibble containing advertising spending data. If `get_times` is TRUE,
#'   the function returns a tibble with date-wise spending data. Otherwise, it returns
#'   a tibble with aggregated spending data, including details like currency, number of ads,
#'   ad type breakdown, advertiser details, and other metrics.
#'
#' @examples
#' # Retrieve aggregated spending data for a specific advertiser in the Netherlands
#' spending_data <- ggl_get_spending(advertiser_id = "AR14716708051084115969",
#'                                   start_date = 20231029, end_date = 20231128,
#'                                   cntry = "NL")
#'
#' # Retrieve time-based spending data for the same advertiser and country
#' time_based_data <- ggl_get_spending(advertiser_id = "AR14716708051084115969",
#'                                     start_date = 20231029, end_date = 20231128,
#'                                     cntry = "NL", get_times = TRUE)
#'
#' @export
ggl_get_spending <- function(advertiser_id,
                             start_date = 20231029, end_date = 20231128,
                             cntry = "NL",
                             get_times = F) {

    if(lubridate::is.Date(start_date)|is.character(start_date)){
        start_date <- lubridate::ymd(start_date) %>% stringr::str_remove_all("-") %>% as.numeric()
    }
    if(lubridate::is.Date(end_date)|is.character(end_date)){
        end_date <- lubridate::ymd(end_date) %>% stringr::str_remove_all("-") %>% as.numeric() %>% magrittr::add(1)
    } else if(is.numeric(end_date)){
        end_date <- end_date + 1
    }

    # statsType <- 2
    # advertiser_id = "AR10605432864201768961"
    cntry_dict <- c(NL = "2528", DE = "2276",
                    BE = "2056", AR = "2032",
                    AU = "2036", BR = "2076",
                    CL = "2152", AT = "2040",
                    BG = "2100", HR = "2191",
                    CY = "2196", CZ = "2203",
                    DK = "2208", EE = "2233",
                    FI = "2246", FR = "2250",
                    GR = "2300", HU = "2348",
                    IE = "2372", IT = "2380",
                    LV = "2428", LT = "2440",
                    LU = "2442", MT = "2470",
                    PL = "2616", PT = "2620",
                    RO = "2642", SK = "2703",
                    SI = "2705", ES = "2724",
                    SE = "2752", IN = "2356",
                    IL = "2376", ZA = "2710",
                    TW = "2158", UK = "2826",
                    US = "2826")

    cntry <- cntry_dict[[cntry]]

    # Define the URL
    url <- "https://adstransparency.google.com/anji/_/rpc/StatsService/GetStats?authuser="

    # Define headers
    headers <- c(
        `accept` = "*/*",
        `accept-language` = "en-US,en;q=0.9",
        `content-type` = "application/x-www-form-urlencoded",
        `sec-ch-ua` = "\"Google Chrome\";v=\"119\", \"Chromium\";v=\"119\", \"Not?A_Brand\";v=\"24\"",
        `sec-ch-ua-mobile` = "?0",
        `sec-ch-ua-platform` = "\"Windows\"",
        `sec-fetch-dest` = "empty",
        `sec-fetch-mode` = "cors",
        `sec-fetch-site` = "same-origin",
        `x-framework-xsrf-token` = "",
        `x-same-domain` = "1")


    # Construct the body
    body <- paste0('f.req={"1":{"1":"', advertiser_id,
                   '","6":', start_date,
                   ',"7":', end_date,
                   ',"8":', jsonlite::toJSON(cntry),
                   '},"3":{"1":2}}')

    # Make the POST request
    response <- httr::POST(url, httr::add_headers(.headers = headers), body = body, encode = "form")

    # Extract the content
    res <- httr::content(response, "parsed")

    ress <- res$`1`

    if(length(ress)==0){
        return(tibble::tibble(spend = 0, number_of_ads = 0))
    }

    dat1 <- ress$`1` %>%
        purrr::flatten() %>%
        purrr::flatten() %>%
        purrr::set_names(c("currency", "spend"))

    dat2 <- ress$`2` %>% tibble::as_tibble() %>% purrr::set_names("number_of_ads")




    dat3 <- ress$`3` %>%
        purrr::map(tibble::as_tibble) %>%
        purrr::map_dfc(~{
            if (.x[3] == "3") {
                purrr::set_names(.x, c("text_ad_perc", "text_ad_spend", "text_type"))
            }  else  if (.x[3] == "2") {
                purrr::set_names(.x, c("img_ad_perc", "img_ad_spend", "img_type"))
            } else   if (.x[3] == "1") {
                purrr::set_names(.x, c("vid_ad_perc", "vid_ad_spend", "vid_type"))
            }
        })

    dat4 <-ress$`5`  %>%
        purrr::flatten() %>%
        purrr::flatten() %>%
        purrr::set_names(c("metric", "advertiser_id", "advertiser_name", "cntry"))

    dat5 <-ress$`6`  %>%
        purrr::flatten() %>%
        purrr::flatten() %>%
        purrr::set_names(c("unk1", "unk2", "unk3"))

    timedat <- ress$`7` %>%
        purrr::map_dfr(tibble::as_tibble) %>%
        purrr::set_names(c("perc_spend", "date")) %>%
        dplyr::mutate(total_spend = as.numeric(dat1$spend)) %>%
        dplyr::mutate(spend = total_spend*perc_spend) %>%
        dplyr::mutate(date = lubridate::ymd(date))

    dat6 <-ress$`8`   %>%
        purrr::flatten() %>%
        purrr::flatten() %>%
        purrr::set_names(c("unk4", "unk5"))

    fin <- dat1 %>%
        tibble::as_tibble() %>%
        dplyr::bind_cols(dat2) %>%
        dplyr::bind_cols(dat3) %>%
        dplyr::bind_cols(dat4) %>%
        dplyr::bind_cols(dat5) %>%
        dplyr::bind_cols(dat6)

    if(get_times){
        return(timedat)
    } else {
        return(fin)
    }

}

# library(tidyverse)
# ggl_get_spending(advertiser_id = "AR18091944865565769729", get_times = T) %>%
#     ggplot(aes(date, spend)) +
#     geom_line()
#
# ggl_get_spending(advertiser_id = "AR18091944865565769729", get_times = F)




#' Fetch a Google Ads Transparency Report
#'
#' Downloads the latest Google Political Ads transparency data bundle (a ZIP file),
#' extracts a specific CSV report, reads it into a tibble, and then cleans
#' up the downloaded and extracted files.
#'
#' @description
#' This function automates the process of obtaining data from the Google Ads
#' Transparency report. It targets the main data bundle, which contains several
#' CSV files. The user can specify which file to process using either its full
#' filename or a convenient shorthand. By default, all downloaded files are
#' deleted after the data is read into memory.
#'
#' @details
#' The data bundle contains several files. The user can specify which file to
#' read using a shorthand alias.
#'
#' \strong{Available Reports (Aliases):}
#' \itemize{
#'   \item \strong{`"creatives"` (Default):} `google-political-ads-creative-stats.csv`. The primary and most detailed file. Contains statistics for each ad creative, including advertiser info, targeting details, and spend.
#'   \item \strong{`"advertisers"`:} `google-political-ads-advertiser-stats.csv`. Aggregate statistics for each political advertiser.
#'   \item \strong{`"weekly_spend"`:} `google-political-ads-advertiser-weekly-spend.csv`. Advertiser spending aggregated by week.
#'   \item \strong{`"geo_spend"`:} `google-political-ads-geo-spend.csv`. Overall spending aggregated by geographic location.
#'   \item \strong{`"advertiser_geo_spend"`:} `google-political-ads-advertiser-geo-spend.csv`. Advertiser-specific spending aggregated by US state.
#'   \item \strong{`"declarations"`:} `google-political-ads-advertiser-declared-stats.csv`. Self-declared information from advertisers in certain regions (e.g., California, New Zealand).
#'   \item \strong{`"advertiser_mapping"`:} `advertiser_id_mapping.csv`. A mapping file to reconcile different advertiser identifiers.
#'   \item \strong{`"creative_mapping"`:} `creative_id_mapping.csv`. A mapping file to reconcile different ad creative identifiers.
#'   \item \strong{`"updated_date"`:} `google-political-ads-updated.csv`. A single-entry file indicating the last time the report data was refreshed.
#'   \item \strong{`"campaigns" (Deprecated):`} `google-political-ads-campaign-targeting.csv`. Ad-level targeting is now in the `"creatives"` file.
#'   \item \strong{`"keywords" (Discontinued):`} `google-political-ads-top-keywords-history.csv`. Historical data on top keywords, terminated in Dec 2019.
#' }
#' For more details on the specific fields in each file, please refer to the
#' Google Ads Transparency Report documentation.
#'
#' @param file_to_read A character string specifying which CSV file to read from
#'   the bundle, using either the full filename or a shorthand alias (e.g., `"creatives"`).
#'   Defaults to `"creatives"`.
#' @param keep_file_at A character path to a directory where the selected CSV file
#'   should be saved. If `NULL` (the default), all downloaded and extracted
#'   files are deleted. If a path is provided, the function will save the
#'   specified `file_to_read` to that location.
#' @param quiet A logical value. If `FALSE` (default), the function will print
#'   status messages about downloading and processing.
#'
#' @return A `tibble` (data frame) containing the data from the selected CSV file.
#'
#' @export
#'
#' @importFrom httr2 request req_user_agent req_perform resp_is_error
#' @importFrom readr read_csv
#' @importFrom fs dir_create file_move path_file
#' @importFrom utils unzip
#' @importFrom cli cli_alert_info cli_alert_success cli_alert_danger cli_abort
#'
#' @examples
#' \dontrun{
#'
#' # Fetch the main creative stats report using the default alias
#' creative_stats <- get_ggl_ads()
#'
#' # Fetch the advertiser stats report using its alias
#' advertiser_stats <- get_ggl_ads(file_to_read = "advertisers")
#'
#' # Fetch the advertiser ID mapping file
#' advertiser_map <- get_ggl_ads(file_to_read = "advertiser_mapping")
#'
#' # Fetch the geo spend report using its full filename
#' geo_spend_report <- get_ggl_ads(
#'   file_to_read = "google-political-ads-geo-spend.csv"
#' )
#'
#' # Fetch the main report and save the CSV file to a "data" folder
#' creative_stats_saved <- get_ggl_ads(
#'   file_to_read = "creatives",
#'   keep_file_at = "data/"
#' )
#' }
get_ggl_ads <-  function(file_to_read = "creatives",
                         keep_file_at = NULL,
                         quiet = FALSE) {

    # --- 1. Argument Validation and Mapping ---
    file_map <- c(
        "creatives" = "google-political-ads-creative-stats.csv",
        "advertisers" = "google-political-ads-advertiser-stats.csv",
        "weekly_spend" = "google-political-ads-advertiser-weekly-spend.csv",
        "geo_spend" = "google-political-ads-geo-spend.csv",
        "advertiser_geo_spend" = "google-political-ads-advertiser-geo-spend.csv",
        "declarations" = "google-political-ads-advertiser-declared-stats.csv",
        "advertiser_mapping" = "advertiser_id_mapping.csv",
        "creative_mapping" = "creative_id_mapping.csv",
        "updated_date" = "google-political-ads-updated.csv",
        "campaigns" = "google-political-ads-campaign-targeting.csv",
        "keywords" = "google-political-ads-top-keywords-history.csv"
    )

    # Resolve user input to a full filename
    if (file_to_read %in% names(file_map)) {
        # User provided a valid alias
        target_filename <- file_map[file_to_read]
    } else if (file_to_read %in% unname(file_map)) {
        # User provided a valid full filename
        target_filename <- file_to_read
    } else {
        # Invalid input
        cli::cli_abort(
            c("x" = "Invalid {.arg file_to_read} specified: {.val {file_to_read}}",
              "i" = "Please use one of the following aliases:",
              "*" = "{.val {names(file_map)}}",
              "i" = "Or provide a full, valid filename.")
        )
    }


    # --- 2. Setup Temporary Environment ---
    # Use a temporary directory to ensure all files are isolated and easily cleaned up
    temp_dir <- tempfile(pattern = "google_ads_")
    fs::dir_create(temp_dir)
    # Ensure the temporary directory is deleted when the function exits,
    # regardless of whether it succeeds or fails.
    on.exit(unlink(temp_dir, recursive = TRUE, force = TRUE), add = TRUE)

    zip_url <- "https://storage.googleapis.com/political-csv/google-political-ads-transparency-bundle.zip"
    zip_path <- file.path(temp_dir, "google-political-ads-transparency-bundle.zip")

    # --- 3. Download the Data Bundle ---
    if (!quiet) cli::cli_alert_info("Downloading data bundle from Google... (This may take a moment)")

    req <- httr2::request(zip_url) |>
        httr2::req_user_agent("metatargetr R package (https://github.com/favstats/metatargetr)")

    resp <- try(httr2::req_perform(req, path = zip_path), silent = TRUE)

    if (inherits(resp, "try-error") || httr2::resp_is_error(resp)) {
        cli::cli_abort("Failed to download the data bundle from Google. Please check your internet connection or the URL: {.url {zip_url}}")
    }

    if (!quiet) cli::cli_alert_success("Download complete. Extracting files...")

    # --- 4. Extract and Read the Specified File ---
    tryCatch({
        utils::unzip(zip_path, exdir = temp_dir)
    }, error = function(e) {
        cli::cli_abort("Failed to unzip the downloaded file. It may be corrupt.")
    })

    target_csv_path <- file.path(temp_dir, target_filename)

    if (!file.exists(target_csv_path)) {
        extracted <- list.files(temp_dir, pattern = "\\.csv$")
        cli::cli_abort(
            c("x" = "The specified file {.file {target_filename}} was not found in the downloaded bundle.",
              "i" = "Available CSV files are: {.file {extracted}}")
        )
    }

    if (!quiet) cli::cli_alert_info("Reading data from {.file {target_filename}}...")

    # Read the CSV into memory
    report_data <- readr::read_csv(target_csv_path, show_col_types = FALSE, progress = !quiet)

    # --- 5. Handle File Persistence ---
    if (!is.null(keep_file_at)) {
        # If the user wants to keep the file, move it from the temp dir to their path
        if (!is.character(keep_file_at) || length(keep_file_at) != 1) {
            cli::cli_warn("{.arg keep_file_at} must be a single directory path. Skipping file save.")
        } else {
            fs::dir_create(keep_file_at)
            final_path <- file.path(keep_file_at, fs::path_file(target_csv_path))
            fs::file_move(target_csv_path, final_path)
            if (!quiet) cli::cli_alert_success("CSV file saved to {.path {final_path}}")
        }
    }

    if (!quiet) cli::cli_alert_success("Processing complete.")

    # The on.exit() call will handle cleanup of the temp_dir automatically
    return(report_data)
}

# get_ggl_ads("google-political-ads-creative-stats.csv")


### FILE:  R/linkedin.R  ###

#' Retrieve Ad Data from the LinkedIn Ad Library with Pagination
#'
#' This function scrapes ad data from the LinkedIn Ad Library, handling pagination
#' to retrieve all available results for a given search query. It first collects
#' all ad detail links and then scrapes each detail page with a configurable
#' timeout and retry mechanism.
#'
#' @param keyword A character string for the keyword to search for (e.g., "Habeck").
#' @param countries A character vector of two-letter country codes (e.g., "DE").
#' @param start_date The start date of the search range in "YYYY-MM-DD" format.
#' @param end_date The end date of the search range in "YYYY-MM-DD" format.
#' @param account_owner Optional. A character string for the ad account owner.
#' @param max_pages The maximum number of pages to scrape. Defaults to 100.
#' @param max_retries The maximum number of retries for each detail page request. Defaults to 5.
#' @param timeout_seconds The timeout in seconds for each detail page request. Defaults to 15.
#'
#' @return A tibble containing the detailed scraped ad information from all pages.
#'
#' @export
#'
#' @examples
#' \dontrun{
#'   ads_data <- get_linkedin_ads(
#'     keyword = "Habeck",
#'     countries = "DE",
#'     start_date = "2025-01-01",
#'     end_date = "2025-02-23",
#'     account_owner = "INSM",
#'     max_pages = 5,
#'     max_retries = 3,
#'     timeout_seconds = 20
#'   )
#'   print(ads_data)
#' }
get_linkedin_ads <- function(keyword,
                             countries,
                             start_date,
                             end_date,
                             account_owner = NULL,
                             max_pages = 100,
                             max_retries = 5,
                             timeout_seconds = 15) {

    # --- 1. Initial Setup ---
    base_search_url <- "https://www.linkedin.com/ad-library/search"
    pagination_url <- "https://www.linkedin.com/ad-library/searchPaginationFragment"

    start_date_str <- tryCatch(format(as.Date(start_date), "%Y-%m-%d"),
                               error = function(e) stop("Invalid start_date format. Please use 'YYYY-MM-DD'."))
    end_date_str <- tryCatch(format(as.Date(end_date), "%Y-%m-%d"),
                             error = function(e) stop("Invalid end_date format. Please use 'YYYY-MM-DD'."))

    query_params <- list(
        keyword = keyword,
        countries = I(countries),
        dateOption = "custom-date-range",
        startdate = start_date_str,
        enddate = end_date_str,
        accountOwner = account_owner
    ) %>% purrr::compact()

    # --- 2. Make Initial Request ---
    message("Fetching first page...")
    req <- httr2::request(base_search_url) %>%
        httr2::req_url_query(!!!query_params) %>%
        httr2::req_user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36") %>%
        httr2::req_timeout(timeout_seconds) %>%
        httr2::req_retry(max_tries = max_retries)

    resp <- tryCatch(httr2::req_perform(req), error = function(e) {
        warning(paste("Failed to retrieve data from LinkedIn. Error:", e$message))
        return(NULL)
    })

    if (is.null(resp)) {
        return(tibble::tibble())
    }

    # --- Helper function to extract token from HTML ---
    extract_pagination_data <- function(html_doc) {
        token_node <- html_doc %>% rvest::html_element("code#paginationMetadata")
        if (is.na(token_node)) return(list(isLastPage = TRUE, paginationToken = NA))

        token_comment <- token_node %>%
            xml2::xml_contents() %>%
            as.character()

        json_str <- stringr::str_remove_all(token_comment, "<!--|-->")

        jsonlite::fromJSON(json_str)
    }

    # --- 3. Collect all detail links, handling pagination ---
    all_detail_paths <- list()
    page_count <- 1

    html_content <- httr2::resp_body_html(resp)
    all_detail_paths[[page_count]] <- html_content %>%
        rvest::html_elements("a[data-tracking-control-name='ad_library_view_ad_detail']") %>%
        rvest::html_attr("href")

    pagination_data <- extract_pagination_data(html_content)
    pagination_token <- pagination_data$paginationToken

    while (!pagination_data$isLastPage && !is.na(pagination_token) && page_count < max_pages) {
        page_count <- page_count + 1
        message(paste("Fetching page", page_count, "..."))

        pag_req <- httr2::request(pagination_url) %>%
            httr2::req_url_query(!!!query_params, paginationToken = pagination_token) %>%
            httr2::req_user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36") %>%
            httr2::req_timeout(timeout_seconds) %>%
            httr2::req_retry(max_tries = max_retries)

        pag_resp <- tryCatch(httr2::req_perform(pag_req), error = function(e) NULL)

        if (is.null(pag_resp)) {
            warning("Failed to fetch a subsequent page. Stopping pagination.")
            break
        }

        html_content <- httr2::resp_body_html(pag_resp)
        all_detail_paths[[page_count]] <- html_content %>%
            rvest::html_elements("a[data-tracking-control-name='ad_library_view_ad_detail']") %>%
            rvest::html_attr("href")

        pagination_data <- extract_pagination_data(html_content)
        pagination_token <- pagination_data$paginationToken
        Sys.sleep(1)
    }

    # --- 4. Scrape each detail page ---
    detail_path <- unlist(all_detail_paths)
    n_ads <- length(detail_path)

    if (n_ads == 0) {
        message("No ads found.")
        return(tibble::tibble())
    }

    print(paste0("Found ", n_ads, " ads across ", page_count, " page(s). Now scraping details..."))

    # Placeholders for undefined helper functions
    if (!exists("parse_ad_details")) {
        parse_ad_details <- function(html) { tibble(url = rvest::html_element(html, "head > link[rel=canonical]") %>% rvest::html_attr("href")) }
    }
    if (!exists("map_dfr_progress")) {
        map_dfr_progress <- purrr::map_dfr
    }

    result_df <- paste0("https://www.linkedin.com", detail_path) %>%
        map_dfr_progress(~{
            url <- .x
            tryCatch({
                # Build a robust request with timeout and retry logic for each detail page
                req_detail <- httr2::request(url) %>%
                    httr2::req_user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36") %>%
                    httr2::req_timeout(timeout_seconds) %>%
                    httr2::req_retry(max_tries = max_retries)

                # Perform the request and parse the response
                resp_detail <- httr2::req_perform(req_detail)

                resp_detail %>%
                    httr2::resp_body_html() %>%
                    parse_ad_details() # This function needs to be defined in your environment

            }, error = function(e) {
                warning(paste("\nCould not process ad detail for URL after retries:", url, "\nFinal Error:", e$message))
                return(NULL) # Return NULL for failed ads
            })
        })

    cat("\nDone.\n")
    return(result_df)
}


#' Parse Important Information from an Ad Detail HTML Page
#'
#' This function takes the HTML content from a LinkedIn Ad Library detail page
#' and extracts key information like advertiser, targeting, and impressions.
#'
#' @param html_content An `xml_document` object, typically read from an HTML file
#'   or obtained from an HTTP response.
#'
#' @return A list containing the extracted ad details.
#'
#' @importFrom rvest html_element html_elements html_text2
#' @importFrom stringr str_trim str_replace
#' @importFrom tibble tibble
#'
#' @export
#'
#' @examples
#' \dontrun{
#'   # Assuming you have saved the HTML of a detail page to "ad_detail.html"
#'   ad_html <- rvest::read_html("ad_detail.html")
#'   details <- parse_ad_details(ad_html)
#'   print(details)
#' }
parse_linkedin_ads_details <- function(html_content) {


    ad_id <- html_content %>%
        rvest::html_element("link[rel='canonical']") %>%
        rvest::html_attr("href") %>%
        stringr::str_extract("\\d+$") # Extracts the numeric ID from the end of the URL

    ad_type <- html_content %>%
        rvest::html_element(".ad-detail-right-rail .text-sm.mb-1") %>%
        rvest::html_text2() %>%
        stringr::str_trim()

    # --- Advertiser and Disclaimer ---
    advertiser_node <- html_content %>%
        rvest::html_element("a[data-tracking-control-name='ad_library_about_ad_advertiser']")

    advertiser <- advertiser_node %>%
        rvest::html_text2() %>%
        stringr::str_trim()

    advertiser_id <- advertiser_node %>%
        rvest::html_attr("href") %>%
        stringr::str_extract("\\d+") # Extracts the numeric ID from the company URL

    paid_by <- html_content %>%
        rvest::html_element(".about-ad__paying-entity") %>%
        rvest::html_text2() %>%
        stringr::str_trim()

    run_dates <- html_content %>%
        rvest::html_element(".about-ad__availability-duration") %>%
        rvest::html_text2() %>%
        stringr::str_trim()

    # --- Ad Content ---
    ad_text <- html_content %>%
        rvest::html_element("p.commentary__content") %>%
        rvest::html_text2() %>%
        stringr::str_trim()

    # --- Impressions ---
    total_impressions <- html_content %>%
        rvest::html_element("div.flex.justify-between > p.font-semibold:last-child") %>%
        rvest::html_text2()

    impression_nodes <- html_content %>%
        rvest::html_elements("li > span.ad-analytics__country-impressions")

    impressions_by_country <- purrr::map_df(impression_nodes, ~{
        country <- .x %>% rvest::html_element("p") %>% rvest::html_text2()
        percentage_text <- .x %>% rvest::html_element("div:last-child p") %>% rvest::html_text2() %>% stringr::str_trim()
        tibble::tibble(country = country, impressions_pct = percentage_text)
    })


    # --- Targeting ---
    # Helper function to find targeting info by its heading, which handles
    # missing info and different ordering.
    find_targeting_info <- function(heading_text) {
        node <<- purrr::detect(targeting_nodes, ~ {
            .x %>% rvest::html_element("h3") %>% rvest::html_text2() == heading_text
        })

        if (!is.null(node)) {
            # Language and Location have a nested span with the value
            selector <- if (heading_text %in% c("Language", "Location")) "p span" else "p"
            return(node %>% rvest::html_element(selector) %>% rvest::html_text2() %>% stringr::str_trim())
        }
        NA_character_ # Return NA if the targeting section isn't found
    }

    targeting_nodes <- html_content  %>%
        rvest::html_elements(".ad-detail-right-rail > div:last-child > div > div")

    language <- find_targeting_info("Language")
    location <- find_targeting_info("Location")
    job_targeting <- find_targeting_info("Job")

    # --- Compile Results into a Tibble ---
    tibble::tibble(
        ad_id = ad_id,
        ad_type = ad_type,
        advertiser = advertiser,
        advertiser_id = advertiser_id,
        paid_by = paid_by,
        run_dates = run_dates,
        ad_text = ad_text,
        total_impressions = total_impressions,
        # Nest the impressions data frame into a single cell
        impressions_by_country = list(impressions_by_country),
        targeting_language = language,
        targeting_location = location,
        targeting_job = job_targeting
    )
}


### FILE:  R/options.R  ###

#' Interactively Set and Save User Settings
#'
#' Launches an interactive command-line interface to help users configure
#' and save default package options, such as the cache directory and
#' user-agent randomization preference.
#'
#' @details
#' The function will guide the user through setting the following options:
#' \itemize{
#'   \item \code{metatargetr.cache_dir}: The default directory to save HTML files.
#'   \item \code{metatargetr.randomize_ua}: Whether to use random User-Agents by default.
#' }
#' The user will also be prompted to save these settings as environment variables
#' in their personal `.Renviron` file for persistence across R sessions.
#'
#' @export
set_metatargetr_options <- function() {
    cli::cli_h1("Configuring settings for {cli::style_bold('metatargetr')}")

    # --- Ask for cache directory ---
    cli::cli_text("{.field First, where should we save downloaded HTML files?}")
    chosen_dir_raw <- readline(prompt = cli::format_inline("Cache directory (press Enter for default: {.path html_cache}): "))
    chosen_dir <- if (identical(chosen_dir_raw, "")) "html_cache" else chosen_dir_raw
    options(metatargetr.cache_dir = chosen_dir)
    cli::cli_alert_success("Cache directory set to: {.path {chosen_dir}}")

    # --- Ask about randomizing user agents ---
    cli::cli_div(theme = list(body = list(`margin-top` = 1))) # Add a blank line
    cli::cli_text("{.field To make scraping more robust, you can randomize the User-Agent for each request.}")
    random_pref <- cli_ask_yes_no("Would you like to enable randomized User-Agents by default?")
    options(metatargetr.randomize_ua = random_pref)
    cli::cli_alert_success("Randomize User-Agents set to: {.val {random_pref}}")

    # --- Ask to save settings permanently ---
    cli::cli_div(theme = list(body = list(`margin-top` = 1)))
    cli::cli_text("{.field These settings apply to your current R session.}")
    save_pref <- cli_ask_yes_no("Do you want to save these settings to your {.path .Renviron} file for future sessions? (recommended)")

    if (save_pref) {
        if (save_pref) {
            set_renv("METATARGETR_CACHE_DIR" = chosen_dir)
            set_renv("METATARGETR_RANDOMIZE_UA" = random_pref)

        }
    }

    # Mark as configured for this session to avoid asking again
    options(metatargetr.configured = TRUE)
    cli::cli_rule(left = "Configuration complete!")
    invisible()
}


### FILE:  R/parse_ad_htmls.R  ###

parse_ad_htmls <- function(
        html_dir = here::here("data", "html_cache"),
        idle_cores = 2
){
    htmls <- list.files(html_dir, full.names = T)
    progressr::handlers(global = TRUE)

    df <- progressr::with_progress({
        p <- progressr::progressor(steps = length(htmls))
        future::plan(future::multisession, workers = parallel::detectCores() - idle_cores)
        htmls %>%
            furrr::future_map_dfr(~{
                end <- read_gz_char(.x) %>%
                    extract_html_and_otherProps()

                p()

                return(end)
            },
            .options = furrr::furrr_options(seed = TRUE)
            )
    })

    # Apply the function to your dataframe
    df_reordered <- reorder_ad_columns(df)

    return(df_reordered)
}


extract_html_and_otherProps <- function(html_txt, marker = "deeplinkAdCard", max_scripts = 20) {
    stopifnot(is.character(html_txt) && length(html_txt) == 1)
    library(xml2)
    library(jsonlite)
    library(tibble)

    # 1. Extract JSON payload as string
    json_str <- get_relevant_json(html_txt, marker = marker, max_scripts = max_scripts)
    if (is.na(json_str) || !nzchar(json_str)) {
        warning("No relevant JSON found")
        return(tibble(html = NA_character_, otherProps = list(list())))
    }
    json_obj <- fromJSON(json_str, simplifyVector = FALSE)

    # 2a. Recursively find the first __html
    find_html_value <- function(x) {
        if (is.list(x)) {
            if ("__html" %in% names(x)) return(x[["__html"]])
            for (item in x) {
                found <- find_html_value(item)
                if (!is.null(found)) return(found)
            }
        }
        NULL
    }

    # 2b. Recursively collect all 'otherProps' elements
    find_all_otherProps <- function(x, acc = list()) {
        if (is.list(x)) {
            if ("otherProps" %in% names(x)) acc <- append(acc, list(x[["otherProps"]]))
            for (item in x) {
                acc <- find_all_otherProps(item, acc)
            }
        }
        acc
    }

    html_val <- find_html_value(json_obj)
    otherProps <- find_all_otherProps(json_obj) %>%
        toJSON() %>%
        fromJSON(flatten = T) %>%
        as_tibble() %>%
        rename_all(function(x){str_remove_all(x, "deeplinkAdCard\\.|snapshot\\.")}) %>%
        mutate_all(as.character)

    # --- Start of the fix ---
    # Identify all columns that need to be gathered
    category_cols <- grep("^page_categories\\.", names(otherProps), value = TRUE)

    # If category columns exist, gather them into a list-column
    if (length(category_cols) > 0) {
        otherProps <- otherProps %>%
            # Use a row-wise operation to gather values
            dplyr::rowwise() %>%
            # Create the new list-column with non-NA category values
            dplyr::mutate(page_categories = list(stats::na.omit(dplyr::c_across(dplyr::all_of(category_cols))))) %>%
            dplyr::ungroup() %>%
            # Remove the original, scattered category columns
            dplyr::select(-dplyr::all_of(category_cols))
    } else {
        # If no category columns were found, add an empty list-column for consistency
        otherProps <- otherProps %>% dplyr::mutate(page_categories = list(NULL))
    }


    category_cols2 <- grep("^branded_content.page_categories\\.", names(otherProps), value = TRUE)

    # If category columns exist, gather them into a list-column
    if (length(category_cols2) > 0) {
        otherProps <- otherProps %>%
            # Use a row-wise operation to gather values
            dplyr::rowwise() %>%
            # Create the new list-column with non-NA category values
            dplyr::mutate(branded_content.page_categories = list(stats::na.omit(dplyr::c_across(dplyr::all_of(category_cols2))))) %>%
            dplyr::ungroup() %>%
            # Remove the original, scattered category columns
            dplyr::select(-dplyr::all_of(category_cols2))
    } else {
        # If no category columns were found, add an empty list-column for consistency
        otherProps <- otherProps %>% dplyr::mutate(branded_content.page_categories = list(NULL))
    }
    # --- End of the fix ---




    # Always return as a tibble, even if NA or empty
    df <- bind_cols(html = if (!is.null(html_val)) html_val else NA_character_,
          otherProps)



    return(df)
}

# get_relevant_json
#  Fast and robust extraction of the JSON payload that contains a marker
#  (defaults to "deeplinkAdCard") from a raw HTML CHARACTER STRING.
#  • xml2 DOM query → no more brittle regex on the whole file
#  • vectorised search   → always returns a SINGLE string or NA
#  • marker is an argument so you can adapt if Meta changes again
get_relevant_json <- function(html_txt,
                              marker      = "deeplinkAdCard",
                              max_scripts = 20) {
    stopifnot(is.character(html_txt), length(html_txt) == 1)

    # 1.  parse once with xml2 (RECOVER avoids choking on FB’s HTML)
    doc <- xml2::read_html(
        html_txt,
        options = c("RECOVER", "NOERROR", "NOWARNING")
    )

    # 2.  collect up to `max_scripts` JSON <script> nodes
    scripts <- xml2::xml_find_all(
        doc,
        sprintf(".//script[@type='application/json'][position()<=%d]", max_scripts)
    )

    if (length(scripts) == 0) return(NA_character_)

    # 3.  pick the FIRST whose text contains the marker
    for (node in scripts) {
        txt <- xml2::xml_text(node)
        if (grepl(marker, txt, fixed = TRUE)) {
            return(txt)           # <- scalar JSON string
        }
    }

    NA_character_              # nothing matched
}



# other version ----
#  locate the *first* JSON script that mentions "deeplinkAdCard" -------------------------------------------------------------------
get_relevant_json_dep <- function(html_txt) {

    stopifnot(is.character(html_txt), length(html_txt) == 1)

    # 1. grab EVERY <script type="application/json">...</script>
    matches <- stringr::str_match_all(
        html_txt,
        "(?is)<script[^>]*type=[\"']application/json[\"'][^>]*>(.*?)</script>"
    )[[1]]              # one matrix; col-2 holds the capture group

    if (nrow(matches) == 0) return(NA_character_)

    # 2. keep only those whose payload contains our marker
    hits <- matches[, 2][stringr::str_detect(matches[, 2], "deeplinkAdCard")]

    if (length(hits) == 0) {
        NA_character_
    } else {
        hits[[1]]         # take the first hit → scalar string
    }
}

# debugonce(parse_ad_htmls)

# EXAMPLE USEAGE:
# df <- parse_ad_htmls("html_cache")
#
# df

reorder_ad_columns <- function(df) {

    # 1. Define the desired order using the correct column names from your data

    # --- Key Identifiers ---
    core_ids <- c(
        "adid", "adArchiveID", "ad_creative_id", "page_id", "pageID", "page_name",
        "pageName", "current_page_name"
    )

    # --- Core Ad Content & Media ---
    ad_content <- c(
        "html", "body.__m", "caption", "title", "link_description", "link_url",
        "byline", "display_format", "mediaType", "cards", "images", "videos",
        "extra_images", "extra_videos", "extra_links", "extra_texts"
    )

    # --- Timeline & Status ---
    timeline <- c(
        "startDate", "endDate", "creation_time", "totalActiveTime",
        "isActive", "activeStatus"
    )

    # --- Spend & Reach ---
    performance <- c(
        "spend", "impressionsWithIndex.impressionsText",
        "impressionsWithIndex.impressionsIndex", "reachEstimate", "currency"
    )

    # --- Targeting & Platforms ---
    delivery <- c(
        "publisherPlatforms", "publisherPlatform", "politicalCountries",
        "targetedOrReachedCountries", "country", "language"
    )

    # --- Disclaimer, Funding & Authorization ---
    funding <- c(
        "disclaimerTexts", "disclaimer_label", "isAAAEligible", "bylines",
        "effective_authorization_category", "funding_entity", "brazil_tax_id"
    )

    # --- Page & Instagram Details ---
    page_details <- c(
        "page_categories", "categories", "page_like_count", "page_entity_type",
        "page_is_profile_page", "page_is_deleted", "instagram_actor_name",
        "instagram_handle", "instagram_profile_pic_url", "instagram_url"
    )

    # 2. Reorder the dataframe using dplyr::select
    df %>%
        select(
            # Select prioritized columns if they exist
            any_of(core_ids),
            any_of(ad_content),
            any_of(timeline),
            any_of(performance),
            any_of(delivery),
            any_of(funding),
            any_of(page_details),

            # Select groups of related columns by their prefix
            starts_with("branded_content"),
            starts_with("instagram_branded_content"),
            starts_with("event"),
            starts_with("fevInfo"),
            starts_with("finServAdData"),
            starts_with("regionalRegulationData"),
            starts_with("dynamic"),

            # Select all remaining columns to place them at the end
            everything()
        )
}


### FILE:  R/parse_interests.R  ###

#' Parse JSON-formatted strings into named character vectors
#'
#' @param include A character vector of JSON-formatted strings
#'
#' @return A list of named character vectors, where each vector represents a parsed JSON object
#'
#' @examples
#' \dontrun{
#' # Parse an example character vector
#' example_json <- c("{\"city\":\"Berlin\",\"zip_code\":\"12345\"}",
#'                   "{\"city\":\"Munich\",\"zip_code\":\"67890\"}")
#' parsed_json <- fix_json(example_json)
#'
#' # Check the resulting list of named character vectors
#' parsed_json
#' }
#'
#' @export
fix_json <- function(include) {
    raw <- include %>%
        stringr::str_replace_all("''", ",") %>%
        paste0("[", ., "]") %>%
        stringr::str_remove_all('\\\\"') %>%
        stringr::str_remove_all('\\\\')

    # print(raw)
    # counttt <<- counttt + 1

    if(raw == "[[]]" | raw == "[{}]" | raw == "[]") raw <- NA

    end <- raw %>%
        purrr::map(~{
            if (!is.na(.x)) {
                parsed_json <- jsonlite::fromJSON(.x)
                unlisted_x <- unlist(parsed_json) %>% na.omit()

                return(paste0(as.character(unlisted_x), ": ", unlisted_x %>% names))
            } else {
                return(NA)
            }
        })

    return(end)
}

#' @title Unnest interest targeting data
#' @description unnest and fix duplicates in interest targeting data from Ad Targeting Dataset
#'
#' The function unnests "include" and "exclude" columns in the Ad Targeting Dataset, and removes duplicates.
#'
#' @param dat a data frame
#' @param the_list the column name to unnest
#' @param new_name the name of the new column after unnesting and fixing duplicates
#' @return a modified data frame with unnested and deduplicated values
#'
#' @examples
#'
#'### example usage:
#'## make sure you have the variable 'archive_id' in your data
#' ad_targeting_data %>%
#'     rowwise() %>%
#'     mutate(include_list = fix_json(include)) %>%
#'     ungroup() %>%
#'     ## the_list: the parsed list of JSON, new_name: what the parsed column should be called
#'    unnest_and_fix_dups(the_list = include_list, new_name = "parsed_include")
#'
#' @export
unnest_and_fix_dups <- function(dat, the_list, new_name) {

    dat %>%
        tidyr::unnest_longer({{the_list}})  %>%
        ### this part is necessary because sometimes there are duplicate targeting criteria
        dplyr::mutate(!!rlang::sym(new_name) := ifelse(
            stringr::str_ends({{the_list}}, "[:digit:]"),
            stringr::str_sub({{the_list}}, 1, nchar({{the_list}})-1),
            {{the_list}})) %>%
        dplyr::group_by(archive_id) %>%
        dplyr::distinct(!!rlang::sym(new_name), .keep_all = T) %>%
        dplyr::ungroup() %>%
        dplyr::select(-{{the_list}})
}




#### example usage:
### make sure you have the variable 'archive_id' in your data
# your_data %>%
#     rowwise() %>%
#     mutate(include_list = fix_json(include)) %>%
#     ungroup() %>%
#     ## the_list: the parsed list of JSON, new_name: what the parsed column should be called
#    unnest_and_fix_dups(the_list = include_list, new_name = "parsed_include")


### FILE:  R/parse_location.R  ###

#' @title Parse Location from Ad Targeting Dataset
#' @description A function to parse the location strings in the Ad Targeting Dataset and split into separate columns for each level of detail.
#' @param .x A data.frame containing the location string
#' @param loc_var A character string specifying the name of the column in .x that contains the location string
#' @param type A character string specifying the prefix to add to each column of split location details. Default is "include". Should be "include" or "exclude".
#' @param verbose A logical flag specifying whether to display a progress bar during processing. Default is `TRUE`.
#' @return A data.frame with columns for each level of detail in the location.
#' @importFrom purrr map_dfr
#' @examples
#' \dontrun{
#'   ### create a dataset with unique include_location values
#'   distinct_data <- targeting_data %>%
#'     distinct(include_location)
#'
#'   #### parse the location data and join in original dataset
#'   distinct_data %>%
#'     parse_location(include_location, type = "include") %>%
#'     right_join(targeting_data)
#'
#'   ###----####
#'
#'   ### create a dataset with unique exclude_location values
#'   distinct_data <- targeting_data %>%
#'     distinct(exclude_location)
#'
#'   #### parse the location data and join in original dataset
#'   distinct_data %>%
#'     parse_location(exclude_location, type = "exclude") %>%
#'     right_join(targeting_data)
#'
#' }
#' @export
parse_location <- function(.x, loc_var, type = "include", verbose = T) {

    if(verbose){
        mapper <- map_dfr_progress
    } else {
        mapper <- purrr::map_dfr
    }

    .x %>%
        dplyr::select({{loc_var}}) %>%
        dplyr::group_by(group = row_number(), .add = T) %>%
        dplyr::group_split() %>%
        mapper(~{

            parse_location_int(.x, {{loc_var}}, type)

        }) %>%
        dplyr::select({{loc_var}}, dplyr::contains("lvl1"), dplyr::contains("lvl2"), dplyr::contains("lvl3"), dplyr::contains("lvl4"))

}

parse_location_int <- function(.x, loc_var, type = "include") {

    val <- .x %>% dplyr::pull({{loc_var}})
    if(is.na(val)) return(NULL)
    json_format <- val %>%
        jsonlite::fromJSON()

    the_depth <- vec_depth(json_format)

    framed <- json_format %>%
        tibble::enframe()


    if(the_depth == 2){
        fin <- framed %>%
            dplyr::rename(lvl1 = name) %>%
            dplyr::select(-value)
    } else if(the_depth == 3){
        fin <- framed %>%
            tidyr::unnest_longer(value) %>%
            dplyr::rename(lvl2 = value_id) %>%
            dplyr::select(-value)%>%
            dplyr::rename(lvl1 = name)
    } else if(the_depth == 4){
        fin <- framed %>%
            tidyr::unnest_longer(value) %>%
            dplyr::rename(lvl2 = value_id) %>%
            tidyr::unnest_longer(value) %>%
            dplyr::rename(lvl3 = value_id) %>%
            tidyr::unnest_longer(value) %>%
            dplyr::rename(lvl1 = name,
                   lvl4 = value)
    }


    fin <-  fin %>%
        dplyr::select(-dplyr::contains("value_id")) %>%
        dplyr::rename_all(~paste0(type, "_location_", .x)) %>%
        dplyr::mutate({{loc_var}} := val)

    return(fin)
}


### FILE:  R/utils-pipe.R  ###

#' Pipe operator
#'
#' See \code{magrittr::\link[magrittr:pipe]{\%>\%}} for details.
#'
#' @name %>%
#' @rdname pipe
#' @keywords internal
#' @export
#' @importFrom magrittr %>%
#' @usage lhs \%>\% rhs
#' @param lhs A value or the magrittr placeholder.
#' @param rhs A function call using the magrittr semantics.
#' @return The result of calling `rhs(lhs)`.
NULL


### FILE:  R/utils.R  ###

utils::globalVariables(c(".", "archive_id", "value", "value_id", "tframe", "ds",
                         "file_name", "filename", "desc", "tag", "total_spend",
                         "perc_spend", "item_type", "event", "event_time",
                         "snapshot", "data", "no_data", "release", "name",
                         "hash", "thefiles", "unique_counter", "hash_table_rows"))

# Helper for yes/no questions, similar to usethis::ui_yeah()
cli_ask_yes_no <- function(question) {
    # Ask the question
    cli::cli_text(question, " (y/n)")

    # Loop until a valid answer is given
    while (TRUE) {
        ans <- tolower(readline("Selection: "))
        if (ans %in% c("y", "yes")) return(TRUE)
        if (ans %in% c("n", "no")) return(FALSE)
        cli::cli_alert_warning("Please answer 'yes' or 'no'.")
    }
}


# Helper function to get a random user agent
get_random_ua <- function() {
    # A short list of common user agents. This could be expanded.
    user_agents <- c(
        "Mozilla", "Chrome", "Motorola",
        "iPhone", "LG", "Safari", "Edge", "Firefox", "Samsung", "Pixel", "OnePlus", "Huawei", "HTC", "Nokia", "Xiaomi", "Redmi", "Oppo", "Vivo", "Lenovo", "Realme", "PlayStation", "Xbox", "Linux", "Macintosh", "Android", "iOS", "Dalvik", "Gecko", "KHTML", "AppleWebKit", "Trident", "EdgeHTML", "SamsungBrowser", "PS5", "iPad", "MacBookPro", "MacIntel", "Win64", "X11", "Ubuntu", "Fedora", "Arch", "Manjaro",
        "wv",
        "PostmanRuntime"
    )
    sample(user_agents, 1)
}

read_gz_char <- function(path) {
    con <- gzfile(path, "rb")
    on.exit(close(con), add = TRUE)
    rawToChar(readBin(con, what = "raw", n = 1e9))   # 1 GB upper bound
}


build_req <- function(id, country, randomize_ua, ua, timeout_sec, retries) {
    url <- glue::glue(
        "https://www.facebook.com/ads/library/?",
        "active_status=all&ad_type=all&country={country}",
        "&id={id}&is_targeted_country=false&media_type=all&search_type=page"
    )


    # Set the user agent for this specific request
    current_ua <- if (randomize_ua) get_random_ua() else ua
    # print(current_ua)
    httr2::request(url) |>
        httr2::req_user_agent(current_ua) |>
        httr2::req_timeout(timeout_sec) |>
        httr2::req_retry(max_tries = retries)
}


walk_progress <- function(.x, .f, ...) {
    .f <- purrr::as_mapper(.f, ...)
    pb <- progress::progress_bar$new(
        total = length(.x),
        format = " (:spin) [:bar] :percent | :current / :total | eta: :eta",
        # format = " downloading [:bar] :percent eta: :eta",
        force = TRUE)

    f <- function(...) {
        pb$tick()
        .f(...)
    }
    purrr::walk(.x, f, ...)
}

map_dfr_progress <- function(.x, .f, ...) {
    .f <- purrr::as_mapper(.f, ...)
    pb <- progress::progress_bar$new(
        total = length(.x),
        format = " (:spin) [:bar] :percent | :current / :total | eta: :eta",
        # format = " downloading [:bar] :percent eta: :eta",
        force = TRUE)

    f <- function(...) {
        pb$tick()
        .f(...)
    }
    purrr::map_dfr(.x, f, ...)
}


### FILE:  tests/testthat.R  ###

# This file is part of the standard setup for testthat.
# It is recommended that you do not modify it.
#
# Where should you do additional test configuration?
# Learn more about the roles of various files in:
# * https://r-pkgs.org/testing-design.html#sec-tests-files-overview
# * https://testthat.r-lib.org/articles/special-files.html

library(testthat)
library(metatargetr)

test_check("metatargetr")


### FILE:  tests/testthat/test-utils.R  ###

test_that("multiplication works", {
  expect_equal(2 * 2, 4)
})


### FILE:  vignettes/google-transparency-report.qmd  ###

---
title: "Google Transparency Report Integration"
description: > 
  Learn how to access and analyze data from the Google Transparency Report.
vignette: >
  %\VignetteIndexEntry{Google Transparency Report Integration}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
knitr:
  opts_chunk: 
    collapse: true
    comment: '#>'
---

## Accessing Google Transparency Data

metatargetr provides integration with the Google Transparency Report through the `ggl_get_spending()` function:

```{r}
#| eval: false
library(metatargetr)
library(ggplot2)

# Get aggregated spending data
spending_data <- ggl_get_spending(
  advertiser_id = "AR18091944865565769729",
  start_date = "2023-10-24",
  end_date = "2023-11-22",
  cntry = "NL"
)
```

## Time-Based Analysis

For temporal analysis, use the `get_times` parameter:

```{r}
#| eval: false
# Get time-series data
timeseries_data <- ggl_get_spending(
  advertiser_id = "AR18091944865565769729",
  start_date = "2023-10-24",
  end_date = "2023-11-22",
  cntry = "NL",
  get_times = TRUE
)

# Create visualization
ggplot(timeseries_data, aes(x = date, y = spend)) +
  geom_col() +
  theme_minimal() +
  labs(
    title = "Ad Spending Over Time",
    x = "Date",
    y = "Spend Amount"
  )
```

## Understanding the Metrics

The Google Transparency Report provides:

- Total spending amounts

- Ad type distribution

- Temporal patterns

- Regional variations


### FILE:  vignettes/introduction.qmd  ###

---
title: "Introduction to metatargetr"
description: > 
  Get started with metatargetr, learn about its core functionality and basic usage.
vignette: >
  %\VignetteIndexEntry{Introduction to metatargetr}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
knitr:
  opts_chunk: 
    collapse: true
    comment: '#>'
---

## Overview

metatargetr is an R package designed to parse and analyze targeting information from the Meta Ad Library dataset and retrieve data from the Audience tab. It provides tools for working with Meta ad library data and includes integration with the Google Transparency Report.

## Installation

You can install the development version of metatargetr from GitHub:

```{r}
#| eval: false
# install.packages("remotes")
remotes::install_github("favstats/metatargetr")
```

## Basic Usage

First, load the package:

```{r}
#| warning: false
#| message: false
library(metatargetr)
```

The package provides several core functions:

1. `get_targeting()`: Retrieve recent targeting data
2. `get_targeting_db()`: Access historical targeting data
3. `get_page_insights()`: Get page information
4. `get_ad_snapshots()`: Download ad creatives
5. `ggl_get_spending()`: Access Google Transparency Report data

Each of these functions is documented in detail in the following sections.


### FILE:  vignettes/retrieve-historical-data.qmd  ###

---
title: "Retrieve Historical Data"
description: > 
  Learn how to access and work with historical Meta Ad targeting data from the archive.
vignette: >
  %\VignetteIndexEntry{Retrieve Historical Data}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
knitr:
  opts_chunk: 
    collapse: true
    comment: '#>'
---

## Accessing Historical Data

While Meta's API only provides recent targeting data, metatargetr maintains an archive of historical targeting data. Access this using `get_targeting_db()`:

```{r}
#| eval: false
library(metatargetr)

# Set parameters
country_code <- "DE"
timeframe <- 30
date <- "2024-10-25"

# Retrieve historical data
historical_data <- get_targeting_db(country_code, timeframe, date)
```

## Understanding Historical Data

The historical data includes:
- Daily targeting snapshots
- Spending information
- Page details
- Targeting criteria used

## Getting Report Data

For broader historical analysis, use `get_report_db()`:

```{r}
#| eval: false
report_data <- get_report_db(
  country_code = "DE",
  timeframe = 30,
  date = "2024-10-25"
)
```

This provides aggregated advertising reports including:

- Total spending

- Number of ads

- Page information

- Temporal data


### FILE:  vignettes/retrieve-targeting-metadata.qmd  ###

---
title: "Retrieve Targeting Metadata"
description: > 
  Understand how to work with targeting metadata and check data availability.
vignette: >
  %\VignetteIndexEntry{Retrieve Targeting Metadata}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
knitr:
  opts_chunk: 
    collapse: true
    comment: '#>'
---

## Overview of Targeting Metadata

The `retrieve_targeting_metadata()` function helps you understand what targeting data is available in the archive. It provides information about:

- Available dates

- File sizes

- Data completeness

- Country coverage


## Basic Usage

```{r}
#| eval: false
library(metatargetr)

# Get metadata for Germany's 30-day data
metadata <- retrieve_targeting_metadata("DE", "30")

# View the metadata
print(metadata)
```

## Understanding the Output

The metadata includes:

- Country code

- Date stamps

- Timeframe information

- Data availability flags

## Using Metadata for Data Validation

```{r}
#| eval: false
# Check data availability before retrieval
metadata <- retrieve_targeting_metadata("DE", "30")

if (nrow(metadata) > 0) {
  # Data is available, proceed with retrieval
  historical_data <- get_targeting_db("DE", 30, metadata$ds[1])
}


### FILE:  vignettes/retrieving-targeting-data.qmd  ###

---
title: "Retrieving Targeting Data"
description: > 
  Learn how to retrieve and analyze Meta Ad targeting data for different time periods.
vignette: >
  %\VignetteIndexEntry{Retrieving Targeting Data}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
knitr:
  opts_chunk: 
    collapse: true
    comment: '#>'
---

## Getting Recent Targeting Data

The `get_targeting()` function allows you to retrieve targeting data for specific time windows:

```{r}
#| eval: false
library(metatargetr)

# Last 7 days
last7 <- get_targeting("121264564551002", timeframe = "LAST_7_DAYS")

# Last 30 days
last30 <- get_targeting("121264564551002", timeframe = "LAST_30_DAYS")

# Last 90 days
last90 <- get_targeting("121264564551002", timeframe = "LAST_90_DAYS")
```

## Understanding the Output

The function returns a data frame containing:

- Basic targeting information (gender, age, location)
- Detailed targeting options (interests, behaviors)
- Custom audience information
- Spending and reach metrics

## Getting Page Information

Use `get_page_insights()` to retrieve additional page information:

```{r}
#| eval: false
page_info <- get_page_insights("121264564551002", include_info = "page_info")
```

This returns details such as:

- Page name and verification status

- Follower counts

- Page category

- Creation date
